# ğŸš€ Custom Prompts in OpenWebUI installieren - Schritt-fÃ¼r-Schritt

## ğŸ“‹ Ãœbersicht

Du hast 3 Custom Prompts verfÃ¼gbar:

| Prompt | Befehl | Zweck |
|--------|--------|-------|
| **Connection Check** | `/openwebui_connection` | Teste API-Verbindung |
| **Models Test** | `/openwebui_models_test` | Teste Modell-Performance |
| **E2E Test** | `/openwebui_e2e_test` | VollstÃ¤ndiger Integration-Test |

---

## âœ… **Schritt 1: OpenWebUI Ã¶ffnen**

```bash
# OpenWebUI ist bereits gestartet auf:
xdg-open http://localhost:3000
```

Oder manuell im Browser: **http://localhost:3000**

---

## âœ… **Schritt 2: API-Verbindung konfigurieren**

**WICHTIG:** Zuerst die API-Verbindung einrichten!

1. **Klicke auf dein Profil-Icon** (rechts oben)
2. **WÃ¤hle "Settings" (âš™ï¸)**
3. **Gehe zu "Connections"**
4. **Unter "OpenAI API":**
   - **API Base URL:** `http://192.168.0.70:8001/v1`
   - **API Key:** (leer lassen)
5. **Klicke "Save & Test"**

âœ… **Du solltest sehen:** "Connection successful"

---

## âœ… **Schritt 3: Prompts installieren**

### **Option A: Via Workspace (Empfohlen)**

1. **Klicke in der linken Sidebar auf "Workspace"** ğŸ“
2. **WÃ¤hle "Functions"**
3. **Klicke auf das **+** Icon** (oben rechts) â†’ "Create New Function"

#### **Prompt 1: Connection Check**

**Kopiere diesen Inhalt:**

```markdown
---
name: OpenWebUI Connection Check
command: /openwebui_connection
description: Teste die Verbindung zwischen OpenWebUI und LocalAgent-Pro API
---

# ğŸ” OpenWebUI Connection Check

Dieser Prompt testet die grundlegende Verbindung zwischen OpenWebUI und LocalAgent-Pro.

## ğŸ“‹ Konfiguration

**API Base URL:**
{{api_base_url | url:placeholder="http://192.168.0.70:8001/v1":default="http://192.168.0.70:8001/v1"}}

**OpenWebUI Port:**
{{openwebui_port | number:placeholder="3000":default="3000"}}

**Modell:**
{{model | select:options=["localagent-pro","llama3.1","tinyllama"]:default="localagent-pro"}}

---

## ğŸ§ª Test-Ablauf

### 1. API Health Check
PrÃ¼fe: `{{api_base_url}}/health`

### 2. Modell-VerfÃ¼gbarkeit
PrÃ¼fe: `{{api_base_url}}/models`
Erwartete Modelle: localagent-pro, {{model}}

### 3. OpenWebUI erreichbar
PrÃ¼fe: `http://localhost:{{openwebui_port}}`

### 4. Test-Request
Sende: "Hallo, bist du bereit?"
An: `{{api_base_url}}/chat/completions`

---

## âœ… Erwartete Ergebnisse

- âœ… Health-Status: "ok"
- âœ… Modelle verfÃ¼gbar: â‰¥2
- âœ… OpenWebUI lÃ¤uft: Status 200
- âœ… Response-Zeit: <1s

---

## ğŸ”§ Bei Problemen

**Connection refused:**
â†’ `cd LocalAgent-Pro && ./start_server.sh`

**Model not found:**
â†’ `ollama pull {{model}}`

**Port-Fehler:**
â†’ PrÃ¼fe OpenWebUI-Container: `docker ps`
```

**EinfÃ¼gen:**
1. FÃ¼ge den Text oben ein
2. **Command:** `/openwebui_connection`
3. **Access:** Public
4. Klicke **"Create"**

---

#### **Prompt 2: Models Test**

**Kopiere diesen Inhalt:**

```markdown
---
name: OpenWebUI Models Test
command: /openwebui_models_test
description: Teste einzelne Modelle auf VerfÃ¼gbarkeit und Performance
---

# ğŸ§ª OpenWebUI Models Test

Testet ein spezifisches Modell auf FunktionalitÃ¤t und Performance.

## ğŸ“‹ Konfiguration

**API Base URL:**
{{api_base_url | url:placeholder="http://192.168.0.70:8001/v1":default="http://192.168.0.70:8001/v1"}}

**Modell:**
{{model | select:options=["localagent-pro","llama3.1","tinyllama"]:default="localagent-pro"}}

**Test-Typ:**
{{test_type | select:options=["smoke-test","health-check","performance","end-to-end"]:default="smoke-test"}}

---

## ğŸ§ª Test-Typen

### **smoke-test** (Schnell)
- âœ… Modell verfÃ¼gbar?
- âœ… Antwortet es?
- â±ï¸ Dauer: ~5s

### **health-check** (Minimal)
- âœ… Nur VerfÃ¼gbarkeit
- â±ï¸ Dauer: ~1s

### **performance** (Detailliert)
- âœ… Response-Zeit
- âœ… Tokens/Sekunde
- âœ… GPU-Nutzung
- â±ï¸ Dauer: ~30s

### **end-to-end** (VollstÃ¤ndig)
- âœ… Alle obigen Tests
- âœ… Tool-AusfÃ¼hrung
- âœ… Streaming
- â±ï¸ Dauer: ~60s

---

## ğŸ¯ Test: {{test_type}}

**Sende an:** `{{api_base_url}}/chat/completions`
**Modell:** `{{model}}`
**Prompt:** "Hallo, was kannst du?"

---

## âœ… Erwartete Ergebnisse

**smoke-test:**
- âœ… Response vorhanden
- âœ… Format korrekt

**performance:**
- âœ… Response-Zeit: <2s
- âœ… Tokens/s: >5 (CPU) / >40 (GPU)
- âœ… GPU aktiv: ja (falls verfÃ¼gbar)

**end-to-end:**
- âœ… Alle Tests bestanden
- âœ… Tools funktionieren
- âœ… Streaming aktiv

---

## ğŸ”§ Bei Problemen

**Langsame Performance (<5 t/s):**
â†’ GPU-Beschleunigung aktivieren:
```bash
cd LocalAgent-Pro
./setup_gpu_acceleration.sh
```

**Modell nicht gefunden:**
â†’ `ollama pull {{model}}`
```

**EinfÃ¼gen wie bei Prompt 1**

---

#### **Prompt 3: E2E Test**

**Kopiere diesen Inhalt:**

```markdown
---
name: OpenWebUI E2E Test
command: /openwebui_e2e_test
description: VollstÃ¤ndiger End-to-End-Test Ã¼ber alle Komponenten
---

# ğŸš€ OpenWebUI End-to-End Test

Testet die komplette Integration: OpenWebUI â†’ LocalAgent-Pro â†’ Ollama â†’ Tools

## ğŸ“‹ Konfiguration

**API Base URL:**
{{api_base_url | url:placeholder="http://192.168.0.70:8001/v1":default="http://192.168.0.70:8001/v1"}}

**OpenWebUI URL:**
{{openwebui_url | url:placeholder="http://localhost:3000":default="http://localhost:3000"}}

**Modell:**
{{model | select:options=["localagent-pro","llama3.1"]:default="localagent-pro"}}

**Test-Prompt:**
{{sample_prompt | textarea:placeholder="z.B. Erstelle Datei test.txt mit Hallo Welt":default="Erstelle Datei test.txt mit Hallo Welt"}}

**Erwartetes Ergebnis:**
{{expected_result | textarea:placeholder="z.B. Datei erstellt, BestÃ¤tigungsnachricht":default="Datei erstellt"}}

---

## ğŸ§ª Test-Ablauf

### 1ï¸âƒ£ **Infrastruktur-Test**
- âœ… Backend lÃ¤uft: `{{api_base_url}}/health`
- âœ… Ollama lÃ¤uft: `systemctl status ollama`
- âœ… OpenWebUI lÃ¤uft: `{{openwebui_url}}`

### 2ï¸âƒ£ **API-Test**
- âœ… `/v1/models` - Modelle verfÃ¼gbar
- âœ… `/v1/chat/completions` - Chat funktioniert

### 3ï¸âƒ£ **OpenWebUI-Integration**
- âœ… Verbindung zu API
- âœ… Modell-Auswahl
- âœ… Chat-Interface

### 4ï¸âƒ£ **Modell-Inferenz**
- ğŸ“ Prompt: `{{sample_prompt}}`
- âœ… Response vorhanden
- âœ… Format korrekt

### 5ï¸âƒ£ **Tool-AusfÃ¼hrung**
- âœ… Tool erkannt
- âœ… Aktion ausgefÃ¼hrt
- âœ… Ergebnis: `{{expected_result}}`

### 6ï¸âƒ£ **Streaming**
- âœ… Streaming aktiv
- âœ… Chunks korrekt

---

## âœ… Erwartete Ergebnisse

**Alle Komponenten:**
- âœ… Backend: Status "ok"
- âœ… Ollama: Active (running)
- âœ… OpenWebUI: HTTP 200

**Integration:**
- âœ… API erreichbar
- âœ… Modelle verfÃ¼gbar
- âœ… Chat funktioniert

**Tool-Execution:**
- âœ… Datei erstellt (falls file-tool)
- âœ… BestÃ¤tigung erhalten
- âœ… Kein Error

**Performance:**
- âœ… Response-Zeit: <3s
- âœ… Tokens/s: >5

---

## ğŸ”§ Bei Problemen

**Backend nicht erreichbar:**
```bash
cd LocalAgent-Pro
./restart_server.sh
```

**Ollama nicht aktiv:**
```bash
systemctl start ollama
```

**OpenWebUI nicht erreichbar:**
```bash
docker restart open-webui
```

**Tool-AusfÃ¼hrung fehlgeschlagen:**
â†’ PrÃ¼fe `config/config.yaml`:
```yaml
sandbox: false  # FÃ¼r direkten Workspace-Zugriff
```

---

## ğŸ“Š Performance-Benchmark

**CPU (ohne GPU):**
- Response-Zeit: 2-5s
- Tokens/s: 5-10

**GPU (mit CUDA):**
- Response-Zeit: 0.5-1s
- Tokens/s: 40-60

**Streaming:**
- Chunks: Kontinuierlich
- Latenz: <100ms
```

**EinfÃ¼gen wie bei Prompt 1 & 2**

---

## âœ… **Schritt 4: Prompts testen**

### **Test 1: Connection Check**
1. Ã–ffne einen neuen Chat in OpenWebUI
2. Tippe: `/openwebui_connection`
3. DrÃ¼cke **Enter**
4. FÃ¼lle die Felder aus (Standard-Werte sind OK)
5. Klicke **Submit**

âœ… **Erwartung:** "âœ… Health-Status: ok, Modelle verfÃ¼gbar: 2"

---

### **Test 2: Models Test**
1. Neuer Chat
2. Tippe: `/openwebui_models_test`
3. **Test-Typ:** smoke-test
4. **Submit**

âœ… **Erwartung:** "âœ… Response vorhanden, Format korrekt"

---

### **Test 3: E2E Test**
1. Neuer Chat
2. Tippe: `/openwebui_e2e_test`
3. **Sample Prompt:** "Erstelle test.txt mit Hello from OpenWebUI"
4. **Expected Result:** "Datei erstellt"
5. **Submit**

âœ… **Erwartung:** "âœ… Tool erkannt, Datei erstellt"

---

## ğŸ¯ **Fertig!**

Du hast jetzt 3 leistungsstarke Test-Prompts in OpenWebUI:

- ğŸ” `/openwebui_connection` - Schneller Health-Check
- ğŸ§ª `/openwebui_models_test` - Performance-Testing
- ğŸš€ `/openwebui_e2e_test` - VollstÃ¤ndiger Workflow-Test

---

## ğŸ“š **Weitere Hilfe**

- **Logs prÃ¼fen:**
  ```bash
  cd LocalAgent-Pro
  tail -f logs/*.log
  ```

- **Server neustarten:**
  ```bash
  ./restart_server.sh
  ```

- **Docker-Container prÃ¼fen:**
  ```bash
  docker ps
  docker logs open-webui
  ```

---

**Viel Erfolg! ğŸš€**
