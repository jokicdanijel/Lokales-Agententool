#!/usr/bin/env python3
"""
VollstÃ¤ndiger Ollama-Integration Test fÃ¼r LocalAgent-Pro
"""

import sys
sys.path.insert(0, 'src')

from ollama_integration import create_ollama_client

print("=" * 80)
print("ğŸ§ª OLLAMA-INTEGRATION TEST")
print("=" * 80)
print()

# Client erstellen
client = create_ollama_client(
    base_url="http://127.0.0.1:11434",
    default_model="llama2"
)

print("\nğŸ“‹ Test 1: Modelle auflisten")
print("-" * 80)
models = client.list_models()
if models:
    print(f"âœ… {len(models)} Modelle gefunden:")
    for model in models:
        name = model.get("name", "unknown")
        size = model.get("size", 0)
        print(f"  â€¢ {name} ({size / 1024 / 1024 / 1024:.2f} GB)")
else:
    print("âŒ Keine Modelle gefunden")

print("\nğŸ§  Test 2: Text-Generierung (Generate)")
print("-" * 80)
response = client.generate(
    prompt="Was ist Docker? Antworte in maximal 2 SÃ¤tzen auf Deutsch.",
    temperature=0.7
)

if response:
    print(f"âœ… Antwort erhalten:")
    print(f"  {response}")
else:
    print("âŒ Keine Antwort erhalten")

print("\nğŸ’¬ Test 3: Chat")
print("-" * 80)
chat_response = client.chat(
    messages=[
        {"role": "system", "content": "Du bist ein hilfreicher deutscher Assistent. Antworte kurz und prÃ¤zise."},
        {"role": "user", "content": "ErklÃ¤re Python in einem Satz."}
    ],
    temperature=0.7
)

if chat_response:
    print(f"âœ… Chat-Antwort erhalten:")
    print(f"  {chat_response}")
else:
    print("âŒ Keine Chat-Antwort erhalten")

print("\n" + "=" * 80)
print("âœ… OLLAMA-INTEGRATION TEST ABGESCHLOSSEN")
print("=" * 80)
print("\nğŸ“Š Log-Dateien prÃ¼fen:")
print("  tail -50 logs/ollama_integration.log")
print("  ./analyze_logs.sh")
