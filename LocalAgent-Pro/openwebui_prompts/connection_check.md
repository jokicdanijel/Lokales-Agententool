# OpenWebUI ‚Äì Verbindungspr√ºfung

**Befehl:** `/openwebui_connection`  
**Zugriff:** `public`

## Eingabeaufforderung

Verifizieren Sie die OpenWebUI-Verbindung und best√§tigen Sie, dass die API-Endpunkte erreichbar sind. F√ºhren Sie eine vollst√§ndige Diagnose durch.

## Eingabefelder

```
{{api_base_url | url:placeholder="z.B. http://127.0.0.1:8001/v1":required}}
{{openwebui_port | number:placeholder="Port (z.B. 3000)":default=3000:required}}
{{model | select:options=["tinyllama","localagent-pro","llama2:latest","llama3.1"]:default="tinyllama":required}}
{{health_endpoint | text:placeholder="Health-Check-Endpunkt":default="/health":required}}
{{description | textarea:placeholder="Zus√§tzliche Hinweise oder Fehlerf√§lle (optional)"}}
```

## Prompt-Template

```
üîç **OpenWebUI-Verbindungstest**

F√ºhre folgende Schritte durch:

1. **API-Erreichbarkeit pr√ºfen:**
   - Teste Health-Check: {{api_base_url}}{{health_endpoint}}
   - Erwarteter Status: 200 OK
   - Pr√ºfe Response-Zeit (< 1s)

2. **Modell-Verf√ºgbarkeit:**
   - Teste: {{api_base_url}}/models
   - Verifiziere, dass "{{model}}" verf√ºgbar ist
   - Liste alle verf√ºgbaren Modelle auf

3. **OpenWebUI-Verbindung:**
   - Pr√ºfe, ob OpenWebUI auf Port {{openwebui_port}} l√§uft
   - Teste Websocket-Verbindung (falls verf√ºgbar)

4. **Diagnose-Output:**
   ‚úÖ **Erfolgreich:** Alle Endpoints erreichbar
   ‚ö†Ô∏è **Warnung:** [Beschreibe Probleme]
   ‚ùå **Fehler:** [Detaillierte Fehlermeldung]

**Zus√§tzliche Hinweise:**
{{description}}

**F√ºhre die Tests aus und gib eine strukturierte Zusammenfassung.**
```

---

## Beispiel-Verwendung

1. In OpenWebUI: Workspace ‚Üí Functions ‚Üí Custom Prompts
2. "New Prompt" klicken
3. Oben stehenden Prompt einf√ºgen
4. Speichern
5. Im Chat: `/openwebui_connection` eingeben und Felder ausf√ºllen

---

## Erwarteter Output

```
üîç OpenWebUI-Verbindungstest - Ergebnisse:

1. ‚úÖ API Health-Check
   - URL: http://127.0.0.1:8001/v1/health
   - Status: 200 OK
   - Response-Zeit: 0.12s
   - Server-Info: {
       "status": "ok",
       "model": "tinyllama",
       "sandbox": true
     }

2. ‚úÖ Modell-Verf√ºgbarkeit
   - Endpoint: http://127.0.0.1:8001/v1/models
   - Verf√ºgbare Modelle:
     ‚Ä¢ tinyllama ‚úÖ (ausgew√§hlt)
     ‚Ä¢ localagent-pro
     ‚Ä¢ llama3.1

3. ‚úÖ OpenWebUI-Verbindung
   - Port 3000: Aktiv
   - Frontend: Erreichbar
   - API-Integration: Funktionsf√§hig

üìä Zusammenfassung:
‚úÖ Alle Tests bestanden
üöÄ System bereit f√ºr Nutzung
```

---

## Troubleshooting

### Problem: "Connection refused"
```bash
# Server-Status pr√ºfen
ps aux | grep openwebui_agent_server

# Server starten (falls nicht l√§uft)
cd /home/danijel-jd/Dokumente/Workspace/Projekte/Lokales\ Agententool/LocalAgent-Pro
./start_server.sh
```

### Problem: "Model not found"
```bash
# Verf√ºgbare Modelle anzeigen
ollama list

# Modell herunterladen
ollama pull tinyllama
```

### Problem: "Health check failed"
```bash
# Logs pr√ºfen
tail -f logs/localagent_pro_main.log

# Ollama-Status
systemctl status ollama
```
