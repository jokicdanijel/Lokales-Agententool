# OpenWebUI â€“ End-to-End-Verbindungstest

**Befehl:** `/openwebui_e2e_test`  
**Zugriff:** `public`

## Eingabeaufforderung

FÃ¼hre einen vollstÃ¤ndigen End-to-End-Test durch, der API-Verbindung, OpenWebUI-Integration, Modell-Inferenz und Tool-AusfÃ¼hrung umfasst.

## Eingabefelder

```
{{api_base_url | url:placeholder="z.B. http://127.0.0.1:8001/v1":required}}
{{openwebui_url | url:placeholder="z.B. http://localhost:3000":required}}
{{model | select:options=["tinyllama","localagent-pro","llama2:latest","llama3.1"]:default="tinyllama":required}}
{{sample_prompt | textarea:placeholder="Beispiel-Prompt zum Testen":default="Erstelle Datei test.txt mit Hallo Welt":required}}
{{expected_result | textarea:placeholder="Was soll das System liefern?":required}}
```

## Prompt-Template

```
ğŸš€ **End-to-End-Test: OpenWebUI â†” LocalAgent-Pro**

Teste die vollstÃ¤ndige Integration von OpenWebUI mit LocalAgent-Pro API.

---

## ğŸ¯ Test-Konfiguration

- **API Base URL:** {{api_base_url}}
- **OpenWebUI URL:** {{openwebui_url}}
- **Modell:** {{model}}
- **Test-Prompt:** {{sample_prompt}}
- **Erwartetes Ergebnis:** {{expected_result}}

---

## ğŸ“‹ Test-Sequenz

### Phase 1: Infrastruktur-Check âœ…
1. **Backend-Server:**
   - PrÃ¼fe: {{api_base_url}}/health
   - Erwarte: Status 200, JSON Response
   - Validiere: `status: "ok"`, `model: "{{model}}"`

2. **Ollama-Service:**
   - PrÃ¼fe: systemctl status ollama
   - Erwarte: active (running)
   - Validiere: GPU-Layers geladen

3. **OpenWebUI-Frontend:**
   - PrÃ¼fe: {{openwebui_url}}
   - Erwarte: UI erreichbar
   - Validiere: Login-Page oder Dashboard

---

### Phase 2: API-Endpoints âœ…
1. **Models-Endpoint:**
   ```bash
   curl -s {{api_base_url}}/models
   ```
   - Erwarte: Liste mit "{{model}}"
   - Validiere: OpenAI-kompatibles Format

2. **Chat-Completions-Endpoint:**
   ```bash
   curl -X POST {{api_base_url}}/chat/completions \
     -H "Content-Type: application/json" \
     -d '{
       "model": "{{model}}",
       "messages": [{"role": "user", "content": "test"}]
     }'
   ```
   - Erwarte: Status 200
   - Validiere: `choices[0].message.content` vorhanden

---

### Phase 3: OpenWebUI-Integration âœ…
1. **API-Konfiguration in OpenWebUI:**
   - Settings â†’ Connections â†’ OpenAI API
   - Base URL: {{api_base_url}}
   - API Key: (optional, kann leer bleiben)
   - Speichern & Testen

2. **Modell-Auswahl:**
   - WÃ¤hle "{{model}}" aus Dropdown
   - Validiere: Modell erscheint in Liste

---

### Phase 4: Funktionstest âœ…
**Test-Prompt:** {{sample_prompt}}

1. **Sende Prompt Ã¼ber OpenWebUI:**
   - Eingabe in Chat: "{{sample_prompt}}"
   - Klicke "Send"

2. **Erwartetes Verhalten:**
   {{expected_result}}

3. **Validierung:**
   - Response innerhalb 5s
   - Keine Fehlermeldungen
   - Ergebnis entspricht Erwartung

---

### Phase 5: Tool-Execution (falls zutreffend) âœ…
**Wenn Prompt Tools nutzt (z.B. Datei erstellen):**

1. **Tool-Detection:**
   - Backend erkennt: "Erstelle Datei"
   - Tool ausgewÃ¤hlt: `write_file()`

2. **Tool-AusfÃ¼hrung:**
   - Datei erstellt in Sandbox
   - Erfolgsmeldung zurÃ¼ck

3. **Verifikation:**
   ```bash
   # Falls Sandbox-Modus:
   ls -lh ~/localagent_sandbox/test.txt
   
   # Falls Live-Modus:
   ls -lh test.txt
   ```

---

## ğŸ“Š Test-Report

Erstelle nach allen Tests folgenden Report:

```markdown
# End-to-End Test-Report
**Datum:** <Timestamp>
**Modell:** {{model}}

## âœ… Erfolgreich
- [x] Backend-Server erreichbar
- [x] Ollama-Service aktiv
- [x] OpenWebUI-Frontend lÃ¤uft
- [x] API-Endpoints funktionieren
- [x] Modell verfÃ¼gbar
- [x] Chat-Integration funktioniert
- [x] Test-Prompt erfolgreich
- [x] Tools ausgefÃ¼hrt (falls zutreffend)

## âš ï¸ Warnungen
<Falls vorhanden, sonst "Keine">

## âŒ Fehler
<Falls vorhanden, sonst "Keine">

## ğŸ“ˆ Performance-Metriken
- Response-Zeit: <X>s
- Tokens/Sekunde: <X> t/s
- GPU-Auslastung: <X>%
- VRAM-Nutzung: <X> MB / 4096 MB

## ğŸ¯ Empfehlung
<Basierend auf Test-Ergebnissen>
```
```

---

## Beispiel-Verwendung

### Test 1: Einfacher Chat-Test

```
/openwebui_e2e_test
API Base URL: http://127.0.0.1:8001/v1
OpenWebUI URL: http://localhost:3000
Modell: tinyllama
Sample Prompt: Hallo, wie geht es dir?
Erwartetes Ergebnis: Freundliche BegrÃ¼ÃŸung, Angebot zur Hilfe
```

**Output:**
```
ğŸš€ End-to-End Test-Report

Phase 1: âœ… Infrastruktur
- Backend: âœ… LÃ¤uft (http://127.0.0.1:8001)
- Ollama: âœ… Active (23/23 GPU Layers)
- OpenWebUI: âœ… Erreichbar (Port 3000)

Phase 2: âœ… API-Endpoints
- /health: 200 OK
- /models: 2 Modelle verfÃ¼gbar
- /chat/completions: Funktioniert

Phase 3: âœ… OpenWebUI-Integration
- API konfiguriert
- Modell "tinyllama" ausgewÃ¤hlt

Phase 4: âœ… Funktionstest
- Prompt: "Hallo, wie geht es dir?"
- Response: "Hallo! Mir geht es gut, danke. Wie kann ich dir heute helfen?"
- Zeit: 1.2s
- Status: âœ… Erfolgreich

ğŸ“Š Performance:
- Response-Zeit: 1.2s
- Tokens/Sekunde: 9.2 t/s
- GPU-Auslastung: 87%

ğŸ¯ Empfehlung: âœ… System voll funktionsfÃ¤hig
```

---

### Test 2: Tool-Integration

```
/openwebui_e2e_test
API Base URL: http://127.0.0.1:8001/v1
OpenWebUI URL: http://localhost:3000
Modell: localagent-pro
Sample Prompt: Erstelle Datei hello.txt mit "Hello World"
Erwartetes Ergebnis: Datei erstellt, BestÃ¤tigung mit Pfad
```

**Output:**
```
ğŸš€ End-to-End Test-Report (Tool-Test)

Phase 1-3: âœ… Alle Checks bestanden

Phase 4: âœ… Tool-AusfÃ¼hrung
- Prompt erkannt: "Erstelle Datei"
- Tool selected: write_file()
- Parameters:
  * path: hello.txt
  * content: "Hello World"

Phase 5: âœ… Tool-Verifikation
- Datei erstellt: ~/localagent_sandbox/hello.txt
- GrÃ¶ÃŸe: 11 bytes
- Inhalt: âœ… Korrekt

Backend-Response:
```
ğŸ¤– LocalAgent-Pro hat deine Anfrage bearbeitet:

âœï¸ Datei schreiben:
âœ… Datei erstellt (Sandbox: /home/user/localagent_sandbox/hello.txt)
ğŸ“ 11 Zeichen geschrieben
```

ğŸ“Š Performance:
- Response-Zeit: 0.3s (Tool-Execution)
- Tool-Latenz: 0.05s
- Gesamt: 0.35s

ğŸ¯ Empfehlung: âœ… Tool-System funktioniert perfekt
```

---

### Test 3: Streaming-Test

```
/openwebui_e2e_test
API Base URL: http://127.0.0.1:8001/v1
OpenWebUI URL: http://localhost:3000
Modell: tinyllama
Sample Prompt: ErklÃ¤re KÃ¼nstliche Intelligenz in 3 SÃ¤tzen
Erwartetes Ergebnis: Streaming-Response, word-by-word
```

**Output:**
```
ğŸš€ End-to-End Test-Report (Streaming)

Phase 1-3: âœ… Alle Checks bestanden

Phase 4: âœ… Streaming-Test
- Request mit "stream": true
- Chunks empfangen: 42
- Format: Server-Sent Events (SSE)
- Anzeige: âœ… Word-by-word in OpenWebUI

Response (vollstÃ¤ndig):
"KÃ¼nstliche Intelligenz (KI) bezeichnet Systeme, die menschenÃ¤hnliche 
kognitive FÃ¤higkeiten simulieren. Sie lernen aus Daten, erkennen Muster 
und treffen Entscheidungen. Anwendungen reichen von Chatbots bis zu 
autonomen Fahrzeugen."

ğŸ“Š Streaming-Metriken:
- Erste Chunk-Latenz: 0.8s
- Durchschnitt pro Chunk: 0.05s
- Gesamt-Zeit: 3.2s
- Chunks: 42

ğŸ¯ Empfehlung: âœ… Streaming funktioniert optimal
```

---

## ğŸ› ï¸ Troubleshooting

### Backend nicht erreichbar
```bash
# Server-Status prÃ¼fen
ps aux | grep openwebui_agent_server

# Logs checken
tail -f logs/localagent_pro_api.log

# Server starten
./start_server.sh
```

### OpenWebUI zeigt "Connection failed"
```bash
# OpenWebUI-Container prÃ¼fen (falls Docker)
docker ps | grep open-webui

# API Base URL prÃ¼fen
curl -s http://127.0.0.1:8001/v1/models

# In OpenWebUI: Settings â†’ Connections
# Base URL: http://127.0.0.1:8001/v1 (mit /v1!)
```

### Modell antwortet nicht
```bash
# Ollama-Status
systemctl status ollama

# Modell laden
ollama pull tinyllama

# GPU-Beschleunigung prÃ¼fen
nvidia-smi
```

### Tools werden nicht ausgefÃ¼hrt
```bash
# Sandbox-Modus prÃ¼fen
cat config/config.yaml | grep sandbox

# Logs fÃ¼r Tool-Execution
tail -f logs/localagent_pro_tools.log

# Test direkt Ã¼ber API
curl -X POST http://127.0.0.1:8001/test \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Erstelle Datei test.txt mit Test"}'
```

---

## ğŸ“š WeiterfÃ¼hrende Dokumentation

- `INSTALLATION.md` - VollstÃ¤ndige Setup-Anleitung
- `GPU_SETUP.md` - GPU-Beschleunigung konfigurieren
- `logs/` - Alle Log-Dateien fÃ¼r Debugging
- `config/config.yaml` - Konfigurationsoptionen
