# OpenWebUI Custom Prompts fÃ¼r LocalAgent-Pro

Diese Sammlung enthÃ¤lt Custom Prompts fÃ¼r OpenWebUI, um die Integration mit LocalAgent-Pro zu testen und zu nutzen.

## ðŸ“‹ VerfÃ¼gbare Prompts

### 1. `/openwebui_connection` - VerbindungsprÃ¼fung
**Datei:** `connection_check.md`

Testet die grundlegende Verbindung zwischen OpenWebUI und LocalAgent-Pro API.

**Was wird getestet:**
- âœ… API Health-Check
- âœ… Modell-VerfÃ¼gbarkeit
- âœ… OpenWebUI-Erreichbarkeit
- âœ… Response-Zeiten

**Verwendung:**
```
/openwebui_connection
API Base URL: http://127.0.0.1:8001/v1
OpenWebUI Port: 3000
Modell: tinyllama
```

---

### 2. `/openwebui_models_test` - Modell-VerfÃ¼gbarkeitstest
**Datei:** `models_test.md`

Testet einzelne Modelle auf VerfÃ¼gbarkeit und Performance.

**Was wird getestet:**
- âœ… Modell-Listing
- âœ… Smoke-Tests
- âœ… Performance-Benchmarks
- âœ… GPU-Beschleunigung

**Test-Typen:**
- **smoke-test**: Schneller Funktionstest
- **health-check**: Nur VerfÃ¼gbarkeit prÃ¼fen
- **performance**: Detaillierte Performance-Messung
- **end-to-end**: VollstÃ¤ndiger Workflow-Test

**Verwendung:**
```
/openwebui_models_test
API Base URL: http://127.0.0.1:8001/v1
Modell: tinyllama
Test-Typ: performance
```

---

### 3. `/openwebui_e2e_test` - End-to-End-Test
**Datei:** `e2e_test.md`

VollstÃ¤ndiger Integration-Test Ã¼ber alle Komponenten.

**Was wird getestet:**
- âœ… Infrastruktur (Backend, Ollama, OpenWebUI)
- âœ… API-Endpoints
- âœ… OpenWebUI-Integration
- âœ… Modell-Inferenz
- âœ… Tool-AusfÃ¼hrung
- âœ… Streaming-Support

**Verwendung:**
```
/openwebui_e2e_test
API Base URL: http://127.0.0.1:8001/v1
OpenWebUI URL: http://localhost:3000
Modell: localagent-pro
Sample Prompt: Erstelle Datei test.txt mit Hallo Welt
Erwartetes Ergebnis: Datei erstellt, BestÃ¤tigungsnachricht
```

---

## ðŸš€ Installation in OpenWebUI

### Schritt 1: OpenWebUI Ã¶ffnen
```bash
# Falls noch nicht gestartet:
docker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui ghcr.io/open-webui/open-webui:main
```

Ã–ffne: http://localhost:3000

### Schritt 2: Workspace Ã¶ffnen
1. Klicke auf **Workspace** (linke Sidebar)
2. WÃ¤hle **Functions** â†’ **Custom Prompts**

### Schritt 3: Prompts hinzufÃ¼gen

#### Variante A: Manuell kopieren
1. Klicke **"New Prompt"**
2. Ã–ffne eine der `.md`-Dateien (z.B. `connection_check.md`)
3. Kopiere den Inhalt unter "Prompt-Template"
4. FÃ¼ge ihn in OpenWebUI ein
5. Konfiguriere:
   - **Command:** `/openwebui_connection`
   - **Access:** `public`
   - **Felder:** Wie in der Datei beschrieben
6. Klicke **Save**

#### Variante B: Import (falls OpenWebUI Import unterstÃ¼tzt)
1. Workspace â†’ Functions â†’ Import
2. WÃ¤hle `.md`-Datei
3. BestÃ¤tige Import

---

## ðŸ“Š Beispiel-Workflows

### Workflow 1: Erstmaliges Setup testen
```
1. /openwebui_connection
   â†’ PrÃ¼fe, ob alles lÃ¤uft

2. /openwebui_models_test (smoke-test)
   â†’ Schneller Funktionstest

3. /openwebui_e2e_test
   â†’ VollstÃ¤ndiger Integration-Test
```

### Workflow 2: Performance-Optimierung
```
1. /openwebui_models_test (performance, tinyllama)
   â†’ Baseline-Performance messen

2. GPU-Beschleunigung aktivieren:
   cd /path/to/LocalAgent-Pro
   ./setup_gpu_acceleration.sh

3. /openwebui_models_test (performance, tinyllama)
   â†’ Vergleiche Ergebnisse (erwarte ~3-4x Speedup)
```

### Workflow 3: Tool-System testen
```
1. /openwebui_e2e_test
   Prompt: "Erstelle Datei hello.txt mit Hello World"
   â†’ Datei-Erstellung testen

2. /openwebui_e2e_test
   Prompt: "Liste alle Dateien auf"
   â†’ Verzeichnis-Listing testen

3. /openwebui_e2e_test
   Prompt: "Lies Datei hello.txt"
   â†’ Datei-Lesen testen
```

---

## ðŸŽ¯ Erwartete Ergebnisse

### Bei funktionierendem System:
```
âœ… connection_check:
   - Alle Endpoints erreichbar
   - Response-Zeit < 1s
   - Modelle verfÃ¼gbar

âœ… models_test (smoke):
   - Modell antwortet
   - Format korrekt
   - Performance: 6-10 t/s (tinyllama, GPU)

âœ… e2e_test:
   - Tools funktionieren
   - Streaming aktiv
   - Keine Fehler
```

### Bei Problemen:
```
âŒ connection_check â†’ "Connection refused"
   â†’ LÃ¶sung: ./start_server.sh

âŒ models_test â†’ "Model not found"
   â†’ LÃ¶sung: ollama pull tinyllama

âŒ e2e_test â†’ "Tool execution failed"
   â†’ LÃ¶sung: config/config.yaml prÃ¼fen (sandbox: true?)
```

---

## ðŸ› ï¸ Konfiguration anpassen

### Standard-Modell Ã¤ndern
Editiere in den `.md`-Dateien:
```
{{model | select:options=["tinyllama","llama3.1","mistral"]:default="llama3.1"}}
```

### API Base URL anpassen
Falls LocalAgent-Pro auf anderem Port lÃ¤uft:
```
{{api_base_url | url:placeholder="z.B. http://127.0.0.1:9000/v1":required}}
```

### Neue Test-Typen hinzufÃ¼gen
In `models_test.md`:
```
{{test_type | select:options=["smoke-test","custom-test"]:default="custom-test"}}

**Bei "custom-test":**
- Beschreibe hier, was passieren soll
- Erwartete Ergebnisse
- Validierungsschritte
```

---

## ðŸ“š Weitere Ressourcen

### LocalAgent-Pro Dokumentation
- `INSTALLATION.md` - Setup-Anleitung
- `GPU_SETUP.md` - GPU-Beschleunigung
- `README.md` - Projekt-Ãœbersicht

### OpenWebUI Dokumentation
- https://docs.openwebui.com
- https://github.com/open-webui/open-webui

### Logs fÃ¼r Debugging
```bash
# API-Logs
tail -f logs/localagent_pro_api.log

# Tool-Logs
tail -f logs/localagent_pro_tools.log

# Ollama-Logs
tail -f logs/localagent_pro_ollama.log

# Alle Logs
tail -f logs/*.log
```

---

## ðŸ”§ Troubleshooting

### Prompt erscheint nicht in OpenWebUI
- PrÃ¼fe: Command startet mit `/`
- PrÃ¼fe: Access ist auf `public` gesetzt
- Neuladen: Strg+R oder Seite neu laden

### Felder werden nicht angezeigt
- Syntax prÃ¼fen: `{{variable | type:placeholder="text":required}}`
- Typen: `url`, `text`, `textarea`, `number`, `select`

### Tests schlagen fehl
```bash
# System-Status komplett prÃ¼fen
./health_check.sh

# Einzelne Komponenten testen
systemctl status ollama
ps aux | grep openwebui_agent_server
nvidia-smi
curl -s http://127.0.0.1:8001/health | jq '.'
```

---

## ðŸ’¡ Tipps & Best Practices

1. **Immer zuerst `connection_check` ausfÃ¼hren**
   - Stellt sicher, dass Basis-Infrastruktur lÃ¤uft

2. **Performance-Tests regelmÃ¤ÃŸig durchfÃ¼hren**
   - Nach System-Updates
   - Nach Config-Ã„nderungen
   - Vergleiche mit Baselines

3. **Logs im Blick behalten**
   - Bei Fehlern: Logs helfen sofort
   - `logs/` Verzeichnis regelmÃ¤ÃŸig checken

4. **Sandbox-Modus nutzen**
   - Verhindert ungewollte SystemÃ¤nderungen
   - Alle Dateien in `~/localagent_sandbox/`

5. **GPU-Beschleunigung aktivieren**
   - 3-4x schneller
   - `./setup_gpu_acceleration.sh`

---

## ðŸ“ Lizenz

Diese Prompts sind Teil von LocalAgent-Pro und unterliegen der gleichen Lizenz.

---

**Erstellt:** November 2025  
**Version:** 1.0  
**Kompatibel mit:** OpenWebUI 0.1.x, LocalAgent-Pro 1.0
