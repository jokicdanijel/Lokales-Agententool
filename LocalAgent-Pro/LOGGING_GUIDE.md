# ðŸ“Š LocalAgent-Pro Logging-System - Komplettanleitung

## ðŸŽ¯ Ãœbersicht

Das LocalAgent-Pro Logging-System bietet umfassendes, strukturiertes Logging fÃ¼r:

- **Backend-API** (Flask-Server auf Port 8001)
- **Tool-AusfÃ¼hrungen** (Dateisystem, Shell, Web)
- **Ollama-Integration** (KI-Modell API-Calls)
- **Request-Tracking** (API-Anfragen und -Antworten)

## ðŸ“ Logging-Architektur

### Erstelle Log-Dateien

```
logs/
â”œâ”€â”€ localagent-pro.log          # Haupt-Log (alle Module)
â”œâ”€â”€ api_requests.log            # API-Request-Tracking
â”œâ”€â”€ tool_executions.log         # Tool-Aufrufe (read_file, write_file, etc.)
â””â”€â”€ ollama_integration.log      # Ollama API-Calls
```

### Log-Rotation

- **Max. DateigrÃ¶ÃŸe**: 10 MB pro Log-Datei
- **Backup-Anzahl**: 5 rotierte Dateien (`.log.1`, `.log.2`, ...)
- **Automatisch**: Logs werden automatisch rotiert bei Erreichen der MaximalgrÃ¶ÃŸe

## ðŸš€ Schnellstart

### 1. Server mit Logging starten

```bash
# Server im Vordergrund mit Console-Logs
python3 src/openwebui_agent_server.py

# ODER: Server im Hintergrund
nohup python3 src/openwebui_agent_server.py > /dev/null 2>&1 &
```

### 2. Logs live verfolgen

```bash
# Interaktives Log-Monitoring
./tail_logs.sh

# Spezifische Log-Datei direkt
./tail_logs.sh localagent-pro

# Alle Logs gleichzeitig
./tail_logs.sh
# WÃ¤hle dann Option [a]
```

### 3. Log-Analyse

```bash
# Detaillierte Statistiken
./analyze_logs.sh
```

**Ausgabe-Beispiel**:
```
ðŸ“Š Log-Level Statistiken:
  DEBUG:    1234
  INFO:     567
  WARNING:  12
  ERROR:    3
  CRITICAL: 0
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL:    1816 EintrÃ¤ge

ðŸ” Top 10 hÃ¤ufigste Meldungen:
  â€¢ API: 456 mal
  â€¢ Tools: 234 mal
  â€¢ Ollama: 123 mal
```

### 4. Log-Cleanup

```bash
# Interaktives Cleanup-MenÃ¼
./cleanup_logs.sh
```

**Optionen**:
- `[1]` Alte Backup-Logs lÃ¶schen (*.log.X)
- `[2]` Alle Logs lÃ¶schen
- `[3]` Nur Backups lÃ¶schen
- `[4]` Logs komprimieren und archivieren

## ðŸ”§ Logging-Konfiguration

### Log-Level Ã¤ndern

**In Python-Code** (`src/openwebui_agent_server.py`):

```python
# Zeile 21-24 Ã¤ndern:
logging_manager = get_logging_manager(
    app_name="LocalAgent-Pro",
    log_level="INFO",  # DEBUG, INFO, WARNING, ERROR, CRITICAL
    console_output=True
)
```

**Empfohlene Settings**:

| Umgebung | Log-Level | Console-Output |
|----------|-----------|----------------|
| Development | `DEBUG` | `True` |
| Testing | `INFO` | `True` |
| Production | `WARNING` | `False` |
| Troubleshooting | `DEBUG` | `True` |

### Log-Format anpassen

**Datei-Format** (in `src/logging_config.py`, Zeile 93):

```python
file_formatter = logging.Formatter(
    '%(asctime)s | %(levelname)-8s | %(name)-20s | %(funcName)-15s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
```

**Console-Format** (Zeile 106):

```python
console_formatter = ColoredFormatter(
    '%(asctime)s | %(levelname_colored)s | %(name)-15s | %(message)s',
    datefmt='%H:%M:%S'
)
```

## ðŸ“Š Log-Level Bedeutung

### DEBUG (Detailliert)
- Alle API-Requests mit vollstÃ¤ndigem Payload
- Tool-Parameter und Ergebnisse
- Datei-Pfade und DateigrÃ¶ÃŸen
- Ollama-Requests und -Responses (gekÃ¼rzt)
- Regex-Pattern-Matching

**Beispiel**:
```
2025-11-16 03:09:27 | DEBUG | LocalAgent-Pro.Tools | read_file | ðŸ” PrÃ¼fe Existenz: /home/sandbox/test.txt
2025-11-16 03:09:27 | DEBUG | LocalAgent-Pro.Tools | read_file | ðŸ“Š DateigrÃ¶ÃŸe: 1234 bytes
```

### INFO (Standard)
- Server-Start und -Konfiguration
- Erfolgreiche API-Requests
- Tool-Aufrufe (welches Tool, Ergebnis)
- Ollama-Calls (Modell, Token-Anzahl, Dauer)

**Beispiel**:
```
2025-11-16 03:09:27 | INFO | LocalAgent-Pro.API | chat_completions | âœ… Chat Completion erfolgreich [a1b2c3d4]: prompt_tokens=45, completion_tokens=123
```

### WARNING (Warnungen)
- Blockierte Domains (Whitelist)
- Nicht gefundene Dateien
- Shell-Kommandos im Sandbox-Modus
- Timeouts (aber erfolgreich recovered)

**Beispiel**:
```
2025-11-16 03:09:27 | WARNING | LocalAgent-Pro.Tools | fetch | ðŸš« Domain blockiert: evil.com (nicht in Whitelist)
```

### ERROR (Fehler)
- API-Request-Fehler
- Tool-Execution-Fehler
- Ollama-Connection-Fehler
- Datei-I/O-Fehler

**Beispiel**:
```
2025-11-16 03:09:27 | ERROR | LocalAgent-Pro.API | chat_completions | âŒ Chat Completion Fehler [xyz123]: 'NoneType' object has no attribute 'get'
```

### CRITICAL (Kritisch)
- Server kann nicht starten
- Config-Datei fehlt
- Kritische Systemfehler

## ðŸ› ï¸ Detailliertes Logging pro Komponente

### 1. API-Request-Logging

**Was wird geloggt**:
- HTTP-Methode und Endpoint
- Request-Payload (gekÃ¼rzt auf 500 Zeichen)
- User-Prompt (gekÃ¼rzt auf 200 Zeichen)
- Response-Status
- Token-Anzahl (Prompt + Completion)
- Request-ID fÃ¼r Tracking

**Beispiel-Logs**:
```
INFO  | LocalAgent-Pro.API | ðŸ“¨ Chat Completion Request [a1b2c3d4] empfangen
DEBUG | LocalAgent-Pro.API | ðŸ“¦ Request Data [a1b2c3d4]: {'messages': [{'role': 'user', 'content': 'Lies Datei test.txt'}], ...
DEBUG | LocalAgent-Pro.API | ðŸ’¬ Anzahl Messages: 1, Modell: localagent-pro
INFO  | LocalAgent-Pro.API | ðŸ‘¤ User Prompt [a1b2c3d4]: Lies Datei test.txt
DEBUG | LocalAgent-Pro.API | ðŸ” Analysiere Prompt fÃ¼r Tool-Erkennung [a1b2c3d4]
INFO  | LocalAgent-Pro.API | âœ… Chat Completion erfolgreich [a1b2c3d4]: prompt_tokens=3, completion_tokens=50
```

### 2. Tool-Execution-Logging

**Tool: read_file**
```
INFO  | LocalAgent-Pro.Tools | ðŸ“– Tool 'read_file' aufgerufen: path=test.txt
DEBUG | LocalAgent-Pro.Tools | ðŸ” PrÃ¼fe Existenz: /home/sandbox/test.txt
DEBUG | LocalAgent-Pro.Tools | ðŸ“Š DateigrÃ¶ÃŸe: 1234 bytes
INFO  | LocalAgent-Pro.Tools | âœ… Datei erfolgreich gelesen: /home/sandbox/test.txt (1234 Zeichen)
DEBUG | LocalAgent-Pro.Tools | ðŸ“„ Content-Vorschau: Hello World...
```

**Tool: write_file**
```
INFO  | LocalAgent-Pro.Tools | âœï¸ Tool 'write_file' aufgerufen: path=output.txt, content_length=100
DEBUG | LocalAgent-Pro.Tools | ðŸ“ Schreibe nach: /home/sandbox/output.txt
DEBUG | LocalAgent-Pro.Tools | ðŸ“„ Content-Vorschau: This is a test file...
INFO  | LocalAgent-Pro.Tools | âœ… Datei erfolgreich geschrieben: /home/sandbox/output.txt (100 Zeichen)
```

**Tool: list_files**
```
INFO  | LocalAgent-Pro.Tools | ðŸ“‚ Tool 'list_files' aufgerufen: path=.
DEBUG | LocalAgent-Pro.Tools | ðŸ” Liste Verzeichnis: /home/sandbox
INFO  | LocalAgent-Pro.Tools | âœ… Verzeichnis aufgelistet: /home/sandbox (12 Dateien, 3 Ordner, 45678 bytes)
```

**Tool: fetch**
```
INFO  | LocalAgent-Pro.Tools | ðŸŒ Tool 'fetch' aufgerufen: url=github.com
DEBUG | LocalAgent-Pro.Tools | ðŸ”§ URL ergÃ¤nzt zu: https://github.com
DEBUG | LocalAgent-Pro.Tools | ðŸ” Extrahierte Domain: github.com
DEBUG | LocalAgent-Pro.Tools | âœ… Domain erlaubt: github.com
DEBUG | LocalAgent-Pro.Tools | ðŸ“¡ Sende HTTP GET Request an: https://github.com
INFO  | LocalAgent-Pro.Tools | âœ… Web-Request erfolgreich: https://github.com (Status: 200, GrÃ¶ÃŸe: 123456 Zeichen)
DEBUG | LocalAgent-Pro.Tools | ðŸ“Š Response Headers: {'Content-Type': 'text/html', ...}
```

**Tool: run_shell**
```
INFO  | LocalAgent-Pro.Tools | ðŸ’» Tool 'run_shell' aufgerufen: cmd=ls -la
WARNING | LocalAgent-Pro.Tools | ðŸš« Shell-Kommando blockiert (Sandbox-Modus aktiv)
```

### 3. Ollama-Integration-Logging

**Generate Request**:
```
INFO  | LocalAgent-Pro.Ollama | ðŸ§  Generate Request [12345678] gestartet
INFO  | LocalAgent-Pro.Ollama | ðŸ“ Model: llama3.1, Temperature: 0.7
DEBUG | LocalAgent-Pro.Ollama | ðŸ‘¤ Prompt [12345678]: Was ist Python? Antworte in einem Satz.
DEBUG | LocalAgent-Pro.Ollama | ðŸ“¦ Payload [12345678]: {'model': 'llama3.1', 'prompt': '...', ...}
DEBUG | LocalAgent-Pro.Ollama | ðŸ“¡ POST http://127.0.0.1:11434/api/generate
DEBUG | LocalAgent-Pro.Ollama | ðŸ“Š Response Status [12345678]: 200
INFO  | LocalAgent-Pro.Ollama | âœ… Generate erfolgreich [12345678]: 45 tokens in 2.34s (19.2 tokens/s)
DEBUG | LocalAgent-Pro.Ollama | ðŸ“Š Details [12345678]: load=0.12s, prompt_tokens=12, response_tokens=45, total=2.34s
DEBUG | LocalAgent-Pro.Ollama | ðŸ’¬ Response [12345678]: Python ist eine vielseitige Programmiersprache...
```

**Chat Request**:
```
INFO  | LocalAgent-Pro.Ollama | ðŸ’¬ Chat Request [87654321] gestartet
INFO  | LocalAgent-Pro.Ollama | ðŸ“ Model: llama3.1, Messages: 2, Temperature: 0.7
DEBUG | LocalAgent-Pro.Ollama | ðŸ’¬ Message 1 [87654321] (system): Du bist ein hilfreicher Assistent.
DEBUG | LocalAgent-Pro.Ollama | ðŸ’¬ Message 2 [87654321] (user): ErklÃ¤re Docker in einem Satz.
DEBUG | LocalAgent-Pro.Ollama | ðŸ“¦ Payload [87654321]: {'model': 'llama3.1', 'messages': [...], ...}
DEBUG | LocalAgent-Pro.Ollama | ðŸ“¡ POST http://127.0.0.1:11434/api/chat
INFO  | LocalAgent-Pro.Ollama | âœ… Chat erfolgreich [87654321]: 52 tokens in 3.12s (16.7 tokens/s)
```

## ðŸ” Troubleshooting mit Logs

### Problem: Server startet nicht

**Log-Datei prÃ¼fen**:
```bash
cat logs/localagent-pro.log | grep CRITICAL
```

**HÃ¤ufige Fehler**:
```
CRITICAL | root | âŒ Config nicht gefunden: /path/to/config.yaml
CRITICAL | root | âŒ Fehler beim Laden der Config: ...
```

**LÃ¶sung**: Config-Datei erstellen oder Pfad korrigieren

### Problem: Tool wird nicht erkannt

**Log-Filter**:
```bash
grep "Tool-Erkennung" logs/localagent-pro.log | tail -20
```

**Was zu suchen**:
- Wurde der Prompt geloggt?
- Welche Regex-Pattern haben matched?
- Welches Tool wurde aufgerufen?

**Debug aktivieren**:
```python
# In openwebui_agent_server.py, Zeile 23
log_level="DEBUG"  # Statt INFO
```

### Problem: Ollama antwortet nicht

**Ollama-Logs prÃ¼fen**:
```bash
# Systemd-Service
journalctl -u ollama -f

# Oder: LocalAgent-Logs
grep "Ollama" logs/ollama_integration.log | tail -20
```

**HÃ¤ufige Fehler**:
```
ERROR | LocalAgent-Pro.Ollama | âŒ Keine Verbindung zu Ollama auf http://127.0.0.1:11434
ERROR | LocalAgent-Pro.Ollama | â° Generate Timeout [xyz] (>60s)
```

**LÃ¶sungen**:
1. Ollama-Service starten: `systemctl start ollama`
2. Timeout erhÃ¶hen in `ollama_integration.py`
3. Modell herunterladen: `ollama pull llama3.1`

### Problem: Langsame Performance

**Performance-Logs analysieren**:
```bash
./analyze_logs.sh
```

**PrÃ¼fen**:
- Wie viele DEBUG-Logs werden geschrieben?
- Wie groÃŸ sind die Log-Dateien?
- Gibt es viele ERROR-Logs (Overhead)?

**Optimierungen**:
1. Log-Level auf INFO setzen (Production)
2. Console-Output deaktivieren
3. Rotating File Handler nutzen (bereits aktiv)

## ðŸ“‹ Systemd-Service mit Logging

### Service-Datei erstellen

```bash
sudo nano /etc/systemd/system/localagent-pro.service
```

**Inhalt**:
```ini
[Unit]
Description=LocalAgent-Pro Backend Server
After=network-online.target ollama.service

[Service]
Type=simple
User=danijel-jd
WorkingDirectory=/home/danijel-jd/Dokumente/Workspace/Projekte/Lokales Agententool/LocalAgent-Pro
ExecStart=/usr/bin/python3 /home/danijel-jd/Dokumente/Workspace/Projekte/Lokales Agententool/LocalAgent-Pro/src/openwebui_agent_server.py
Restart=always
RestartSec=3

# Logging
StandardOutput=journal
StandardError=journal
SyslogIdentifier=localagent-pro

[Install]
WantedBy=multi-user.target
```

### Service aktivieren

```bash
sudo systemctl daemon-reload
sudo systemctl enable localagent-pro
sudo systemctl start localagent-pro
```

### Logs ansehen

```bash
# Live-Logs
journalctl -u localagent-pro -f

# Logs seit heute
journalctl -u localagent-pro --since today

# Letzte 100 Zeilen
journalctl -u localagent-pro -n 100

# Nur ERROR-Logs
journalctl -u localagent-pro -p err
```

## ðŸ” Sicherheit & Datenschutz

### Sensible Daten maskieren

**Automatisch maskiert**:
- PasswÃ¶rter (`password`, `passwd`)
- API-Keys (`api_key`, `apikey`)
- Tokens (`token`, `access_token`)
- Secrets (`secret`)

**Beispiel**:
```python
# Vor Logging:
"password: secret123, api_key: abc-def-ghi"

# Nach Maskierung:
"password: ***MASKED***, api_key: ***MASKED***"
```

### Eigene Maskierungs-Patterns

**In `logging_config.py`**:
```python
# Zeile 311-318 erweitern:
patterns = [
    (r'password["\s:=]+([^"\s,]+)', 'password: ***MASKED***'),
    (r'api[_-]?key["\s:=]+([^"\s,]+)', 'api_key: ***MASKED***'),
    (r'token["\s:=]+([^"\s,]+)', 'token: ***MASKED***'),
    (r'secret["\s:=]+([^"\s,]+)', 'secret: ***MASKED***'),
    # Eigene Pattern hier hinzufÃ¼gen:
    (r'email["\s:=]+([^"\s,]+)', 'email: ***MASKED***'),
]
```

### Log-Dateien schÃ¼tzen

```bash
# Nur Owner kann lesen
chmod 600 logs/*.log

# Log-Verzeichnis schÃ¼tzen
chmod 700 logs/
```

## ðŸ“ˆ Performance-Optimierung

### 1. Log-Level anpassen

**Development**:
```python
log_level="DEBUG"  # Alle Details
```

**Production**:
```python
log_level="INFO"   # Nur wichtige Events
```

**High-Load Production**:
```python
log_level="WARNING"  # Nur Warnungen und Fehler
```

### 2. Content-KÃ¼rzung konfigurieren

**In `logging_config.py`**, Funktion `truncate_long_content`:

```python
# Zeile 336-348: max_length anpassen
def truncate_long_content(content: str, max_length: int = 1000) -> str:
    # FÃ¼r Production: max_length = 200
    # FÃ¼r Debugging: max_length = 5000
```

### 3. Rotation-Settings

**Mehr Speicher, weniger I/O**:
```python
# Zeile 24-25 in logging_config.py
max_file_size: int = 50 * 1024 * 1024,  # 50 MB
backup_count: int = 10
```

**Weniger Speicher, mehr Rotation**:
```python
max_file_size: int = 5 * 1024 * 1024,   # 5 MB
backup_count: int = 3
```

## ðŸ§ª Testing

### Logging-Modul testen

```bash
# Standalone-Test
python3 src/logging_config.py

# Ollama-Integration testen
python3 src/ollama_integration.py
```

### VollstÃ¤ndiger Server-Test mit Logging

```bash
# Server starten
python3 src/openwebui_agent_server.py &
SERVER_PID=$!

# Logs live verfolgen (in neuem Terminal)
./tail_logs.sh

# API testen
curl http://127.0.0.1:8001/health
curl -X POST http://127.0.0.1:8001/test -H "Content-Type: application/json" -d '{"prompt":"Liste alle Dateien auf"}'

# Server stoppen
kill $SERVER_PID
```

## ðŸ“š WeiterfÃ¼hrende Ressourcen

- **Python Logging Docs**: https://docs.python.org/3/library/logging.html
- **Log-Rotation**: https://docs.python.org/3/library/logging.handlers.html#rotatingfilehandler
- **Systemd Logging**: `man journalctl`

## ðŸ†˜ HÃ¤ufige Fragen (FAQ)

### Q: Logs werden nicht erstellt?

**A**: PrÃ¼fe Berechtigungen:
```bash
mkdir -p logs
chmod 755 logs
```

### Q: Zu viele Logs, Festplatte voll?

**A**: Nutze Cleanup-Skript:
```bash
./cleanup_logs.sh
# WÃ¤hle Option [4] fÃ¼r Archivierung
```

### Q: Logs in Datei UND Console?

**A**: In `openwebui_agent_server.py`:
```python
console_output=True  # Zeile 24
```

### Q: Nur ERROR-Logs anzeigen?

**A**: Mit grep filtern:
```bash
grep " ERROR \| CRITICAL " logs/localagent-pro.log
```

### Q: Logs nach Zeitraum filtern?

**A**: Mit grep und Zeitstempel:
```bash
# Heute zwischen 10:00 und 11:00
grep "2025-11-16 10:" logs/localagent-pro.log
```

---

**âœ… Logging-System erfolgreich implementiert!**

FÃ¼r weitere Hilfe: Siehe `README.md` oder erstelle ein Issue auf GitHub.
