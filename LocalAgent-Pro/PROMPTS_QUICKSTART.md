# ðŸš€ CUSTOM PROMPTS - COPY & PASTE FÃœR OPENWEBUI

## SO INSTALLIERST DU DIE PROMPTS

1. **Ã–ffne OpenWebUI:** <http://localhost:3000>
2. **Klicke:** Workspace (linke Sidebar) â†’ Functions â†’ **+** Icon
3. **WÃ¤hle:** "Create New Function"
4. **Kopiere** einen der Prompts unten
5. **FÃ¼ge ein** und klicke **"Create"**

---

## âœ… PROMPT 1: CONNECTION CHECK

**In OpenWebUI einfÃ¼gen:**

```
Teste die Verbindung zwischen OpenWebUI und LocalAgent-Pro API.

FÃ¼hre folgende Schritte aus:

1. **API Health Check:**
   - URL: http://192.168.0.70:8001/health
   - Erwarte: Status 200, { "status": "ok" }

2. **Modelle prÃ¼fen:**
   - URL: http://192.168.0.70:8001/v1/models
   - Erwarte: localagent-pro, llama3.1

3. **Test-Request:**
   - Sende: "Hallo, bist du bereit?"
   - Erwarte: Erfolgreiche Response < 2s

**Gib strukturierten Report:**
âœ… Health: ok
âœ… Modelle: 2 verfÃ¼gbar
âœ… Response-Zeit: Xs
âœ… System: Bereit
```

**Konfiguration:**

- **Name:** Connection Check
- **Command:** `/connection`
- **Access:** Public

---

## âœ… PROMPT 2: MODEL TEST

**In OpenWebUI einfÃ¼gen:**

```
Teste das Modell "localagent-pro" auf FunktionalitÃ¤t.

**Test-Schritte:**

1. **VerfÃ¼gbarkeit:**
   - PrÃ¼fe: Modell in /v1/models Liste
   - Status: VerfÃ¼gbar?

2. **Smoke-Test:**
   - Prompt: "Was kannst du?"
   - Erwarte: Klare Antwort
   - Zeit: < 3s

3. **Performance:**
   - Messe: Tokens/Sekunde
   - GPU: Aktiv?
   - Benchmark: > 5 t/s

**Gib Report:**
âœ… VerfÃ¼gbar: Ja/Nein
âœ… Response-Zeit: Xs
âœ… Tokens/s: X t/s
âœ… GPU: Aktiv/Inaktiv
```

**Konfiguration:**

- **Name:** Model Test
- **Command:** `/modeltest`
- **Access:** Public

---

## âœ… PROMPT 3: E2E TEST

**In OpenWebUI einfÃ¼gen:**

```
VollstÃ¤ndiger End-to-End-Test: OpenWebUI â†’ LocalAgent-Pro â†’ Ollama

**Test-Sequenz:**

1. **Infrastruktur:**
   - Backend: http://192.168.0.70:8001/health
   - Ollama: LÃ¤uft?
   - OpenWebUI: http://localhost:3000

2. **API-Integration:**
   - Verbindung: Erfolgreich?
   - Modell: localagent-pro verfÃ¼gbar?

3. **Funktionstest:**
   - Sende: "Erstelle Datei test.txt mit Hallo"
   - Erwarte: Tool ausgefÃ¼hrt, Datei erstellt

4. **Streaming:**
   - Test: Word-by-word Anzeige
   - Latenz: < 100ms pro Chunk

**Report:**
âœ… Backend: Status
âœ… API: Verbunden
âœ… Tools: Funktionieren
âœ… Streaming: Aktiv
âœ… Performance: X t/s
```

**Konfiguration:**

- **Name:** E2E Test
- **Command:** `/e2etest`
- **Access:** Public

---

## ðŸŽ¯ SCHNELLSTART

### Nach Installation der Prompts

1. **Teste Verbindung:**

   ```
   /connection
   ```

   âœ… Sollte zeigen: "System bereit"

2. **Teste Modell:**

   ```
   /modeltest
   ```

   âœ… Sollte zeigen: "Tokens/s: ~9 t/s"

3. **Teste Tool-System:**

   ```
   /e2etest
   ```

   âœ… Sollte Datei erstellen

---

## ðŸ”§ ALTERNATIVE: DIREKT TESTEN (OHNE CUSTOM PROMPTS)

Falls du die Prompts nicht installieren mÃ¶chtest, kannst du direkt testen:

### Test 1: Chat

```
Hallo, bist du bereit?
```

### Test 2: Datei erstellen

```
Erstelle Datei hello.txt mit "Hello from OpenWebUI"
```

### Test 3: Datei lesen

```
Lies Datei hello.txt
```

### Test 4: Verzeichnis

```
Liste alle Dateien auf
```

---

## âœ… API-VERBINDUNG (WICHTIG!)

**Stelle sicher, dass in OpenWebUI konfiguriert ist:**

```
Settings â†’ Connections â†’ OpenAI API

API Base URL: http://192.168.0.70:8001/v1
API Key: (leer lassen)

â†’ Save & Test
â†’ Sollte zeigen: "Connection successful"
```

---

## ðŸ“Š ERWARTETE ERGEBNISSE

### Connection Test

```
âœ… Health: ok
âœ… Modelle: localagent-pro, llama3.1
âœ… Response-Zeit: 0.2s
âœ… System: Bereit
```

### Model Test

```
âœ… VerfÃ¼gbar: Ja
âœ… Response-Zeit: 1.2s
âœ… Tokens/s: 9.2 t/s (GPU aktiv)
âœ… GPU: NVIDIA GTX 1050 - 87% Auslastung
```

### E2E Test

```
âœ… Backend: LÃ¤uft auf 192.168.0.70:8001
âœ… API: Verbunden
âœ… Tool: write_file() ausgefÃ¼hrt
âœ… Datei: test.txt erstellt (5 bytes)
âœ… Streaming: Aktiv (42 Chunks)
âœ… Performance: 9.2 t/s
```

---

## ðŸ†˜ PROBLEME?

### "Connection failed"

```bash
cd LocalAgent-Pro
./restart_server.sh
```

### "Model not found"

```bash
ollama pull llama3.1
```

### "Slow response"

```bash
nvidia-smi  # GPU prÃ¼fen
```

---

**Du bist bereit! Viel Erfolg! ðŸš€**
