# LocalAgent-Pro â†” ELION Hyper-Dashboard Integration

## ğŸ¯ Architektur-Positionierung

**LocalAgent-Pro** kann als **opena21** in das ELION-System integriert werden oder als eigenstÃ¤ndiger **Inference-Service** auf Port 8001 parallel laufen.

### Integration-Optionen

#### Option 1: Als opena21 (Inference-Agent) - Port 12364
```yaml
Agent: opena21 (AI-Inference)
Port: 12364
Rolle: LLM-basierter Inference-Service
Status: Online
Beschreibung: Lokaler AI-Agent mit Ollama (llama3.1), Tool-Execution, Sandbox
VerknÃ¼pfungen: 
  - EmpfÃ¤ngt AuftrÃ¤ge von opena1 (Koordinator)
  - Schreibt Ergebnisse via opena2 (Archivator)
  - Stellt /metrics fÃ¼r opena20 (Dashboard) bereit
```

#### Option 2: EigenstÃ¤ndig - Port 8001 (Aktuell)
```yaml
Service: LocalAgent-Pro (Standalone)
Port: 8001
Rolle: OpenWebUI-kompatibler AI-Agent
Status: Online
Beschreibung: UnabhÃ¤ngiger LLM-Service mit direkter OpenWebUI-Integration
Integration:
  - Prometheus-Metriken fÃ¼r opena20 (Dashboard)
  - Kann von opena3 (OpenWebUI) direkt angesprochen werden
  - LÃ¤uft parallel zum ELION-System
```

---

## ğŸ”— Integration in ELION-Architektur

### 1. Kommunikation mit Koordinator (opena1)

**LocalAgent-Pro â†’ opena1 (Request Flow):**
```python
# LocalAgent empfÃ¤ngt Chat-Request
POST /v1/chat/completions
â†“
# Tool-Execution + LLM-Inference
analyze_and_execute(prompt)
â†“
# Ergebnis an Koordinator melden (optional)
POST http://localhost:12344/api/task/complete
{
    "task_id": "uuid",
    "result": "...",
    "safepoint": true
}
```

**opena1 â†’ LocalAgent-Pro (Dispatch):**
```python
# Koordinator dispatched AI-Aufgabe
POST http://localhost:8001/v1/chat/completions
{
    "messages": [
        {"role": "system", "content": "Du bist ein Tool-Agent..."},
        {"role": "user", "content": "Erstelle Datei report.txt mit..."}
    ],
    "metadata": {
        "dispatcher": "opena1",
        "task_id": "uuid",
        "archive_to": "opena2"
    }
}
```

### 2. Archivierung Ã¼ber opena2

**Safepoint-Integration:**
```python
# Nach erfolgreicher Tool-Execution
def archive_result_to_opena2(result):
    safepoint = {
        "type": "AI_INFERENCE",
        "agent": "localagent-pro",
        "timestamp": int(time.time()),
        "input": user_prompt,
        "output": result,
        "tools_used": ["write_file", "read_file"],
        "sandbox_path": "/home/danijel-jd/localagent_sandbox"
    }
    
    requests.post(
        "http://localhost:12345/api/safepoint",
        json=safepoint
    )
```

**Deduplizierung:** opena2 prÃ¼ft, ob identische Requests bereits existieren (via MD5-Hash).

### 3. Dashboard-Integration (opena20)

**Metriken-Export:**
- LocalAgent-Pro stellt bereits `/metrics` bereit
- opena20 kann diese scrapen fÃ¼r:
  - Request Rate
  - Error Rate
  - Ollama Performance
  - Loop Detections
  - Tool Usage

**Dashboard-Queries:**
```promql
# LocalAgent-Pro Performance
rate(localagent_requests_total{job="localagent-pro"}[5m])

# Fehlerrate
rate(localagent_requests_total{status="error"}[5m]) / rate(localagent_requests_total[5m])

# Loop-Protection Aktivierungen
increase(localagent_loop_detections_total[1h])
```

### 4. OpenWebUI-Integration (opena3)

**Direkte Anbindung:**
```
OpenWebUI (opena3:8080)
    â†“
    API Base URL: http://localhost:8001/v1
    â†“
LocalAgent-Pro (Port 8001)
    â†“
    Ollama (llama3.1)
```

**Vorteile:**
- âœ… OpenWebUI kann LocalAgent-Pro als "Custom API" nutzen
- âœ… Alle Tool-Executions laufen Ã¼ber Sandbox
- âœ… Volle OpenAI-KompatibilitÃ¤t

---

## ğŸ› ï¸ Implementierungs-Schritte

### Schritt 1: Registry-Eintrag erstellen

**Datei:** `configs/agent_registry.yaml` (im ELION-Projekt)

```yaml
agents:
  # ... bestehende Agenten ...
  
  opena21:
    name: "AI-Inference (LocalAgent-Pro)"
    port: 12364  # ODER 8001 fÃ¼r Standalone
    host: "localhost"
    role: "inference"
    status: "online"
    health_endpoint: "/health"
    metrics_endpoint: "/metrics"
    capabilities:
      - "llm_inference"
      - "tool_execution"
      - "file_operations"
      - "web_fetch"
      - "sandbox_isolation"
    dependencies:
      - "opena1"  # Koordinator
      - "opena2"  # Archivator
    integration:
      protocol: "http"
      api_version: "openai_v1"
      model: "llama3.1"
```

### Schritt 2: Koordinator-Routing konfigurieren

**opena1 Dispatch-Rules:**

```python
# In opena1 (Koordinator)
AGENT_ROUTES = {
    "ai_inference": "http://localhost:8001/v1/chat/completions",
    "tool_execution": "http://localhost:8001/test",
    # ...
}

def dispatch_ai_task(task):
    """Leitet AI-Aufgaben an LocalAgent-Pro"""
    response = requests.post(
        AGENT_ROUTES["ai_inference"],
        json={
            "messages": task["messages"],
            "metadata": {
                "task_id": task["id"],
                "dispatcher": "opena1"
            }
        }
    )
    
    # Ergebnis an Archivator
    if response.ok:
        archive_to_opena2(task["id"], response.json())
    
    return response.json()
```

### Schritt 3: Prometheus-Scraping erweitern

**Bereits erledigt!** LocalAgent-Pro ist bereits in Prometheus konfiguriert:

```yaml
# /home/danijel-jd/Dokumente/Workspace/Projekte/Gesamtprojekt/configs/prometheus.yaml
scrape_configs:
  - job_name: 'localagent-pro'
    metrics_path: '/metrics'
    scrape_interval: 15s
    static_configs:
      - targets: ['172.17.0.1:8001']
        labels:
          service: 'localagent-pro'
          environment: 'production'
```

**opena20 (Dashboard) kann diese Metriken nutzen!**

### Schritt 4: Telegram-Bot Integration (opena4)

**LocalAgent-Pro via Telegram nutzen:**

```python
# In opena4 (Telegram-Bot)
def handle_ai_request(message):
    """Leitet Telegram-Nachricht an LocalAgent-Pro"""
    
    # 1. Schreibe in Archiv (opena2)
    msg_id = archive_message(message)
    
    # 2. Rufe LocalAgent-Pro
    response = requests.post(
        "http://localhost:8001/v1/chat/completions",
        json={
            "messages": [
                {"role": "user", "content": message.text}
            ],
            "metadata": {
                "source": "telegram",
                "chat_id": message.chat.id,
                "msg_id": msg_id
            }
        }
    )
    
    # 3. Archiviere Antwort (opena2)
    archive_response(msg_id, response.json())
    
    # 4. Sende an Telegram zurÃ¼ck
    bot.send_message(message.chat.id, response.json()["choices"][0]["message"]["content"])
```

---

## ğŸ“Š Monitoring & Health-Checks

### Dashboard-Integration (opena20)

**LocalAgent-Pro Metriken in ELION-Dashboard:**

```javascript
// opena20 Dashboard-Panel
{
  "title": "AI-Inference Performance",
  "panels": [
    {
      "query": "rate(localagent_requests_total[5m])",
      "title": "LocalAgent Request Rate"
    },
    {
      "query": "localagent_active_requests",
      "title": "Active Requests"
    },
    {
      "query": "rate(localagent_ollama_calls_total{status='success'}[5m])",
      "title": "Ollama Success Rate"
    },
    {
      "query": "localagent_loop_detections_total",
      "title": "Loop Protection (Critical!)"
    }
  ]
}
```

### Health-Check-Routing

**opena20 â†’ LocalAgent-Pro:**
```bash
# Periodischer Health-Check
curl http://localhost:8001/health

# Response:
{
  "status": "ok",
  "model": "llama3.1",
  "sandbox": true,
  "sandbox_path": "/home/danijel-jd/localagent_sandbox"
}
```

**Alert-Regeln:**
```yaml
# In opena20 oder Prometheus
- alert: LocalAgentDown
  expr: up{job="localagent-pro"} == 0
  for: 1m
  annotations:
    summary: "LocalAgent-Pro ist offline!"

- alert: LocalAgentHighErrorRate
  expr: rate(localagent_requests_total{status="error"}[5m]) > 0.1
  for: 5m
  annotations:
    summary: "LocalAgent Error Rate > 10%"
```

---

## ğŸ” Security-Integration

### Sandbox-Isolation

**LocalAgent-Pro nutzt bereits Sandbox:**
```yaml
sandbox: true
sandbox_path: /home/danijel-jd/localagent_sandbox
```

**FÃ¼r ELION-Integration:**
- Alle File-Operationen laufen in Sandbox
- Shell-Commands sind blockiert (sandbox-mode)
- Nur whitelisted Domains erlaubt

### Unlock-Master Integration (opena11)

**VerschlÃ¼sselte Secrets via opena11:**
```python
# LocalAgent-Pro ruft opena11 fÃ¼r Credentials
def get_api_key(service):
    response = requests.post(
        "http://localhost:12354/api/unlock",
        json={"service": service, "requester": "localagent-pro"}
    )
    return response.json()["key"]

# Nutzung in LocalAgent
api_key = get_api_key("openai")  # Falls externe APIs genutzt werden
```

---

## ğŸš€ Deployment-Szenarien

### Szenario 1: Standalone (Aktuell)

```
LocalAgent-Pro (Port 8001)
    â”œâ”€â”€ Prometheus Monitoring âœ…
    â”œâ”€â”€ OpenWebUI Integration âœ…
    â””â”€â”€ LÃ¤uft parallel zu ELION

ELION-System (Ports 12344-12363)
    â”œâ”€â”€ opena1 (Koordinator)
    â”œâ”€â”€ opena2 (Archivator)
    â”œâ”€â”€ opena3 (OpenWebUI)
    â”œâ”€â”€ opena4 (Telegram)
    â””â”€â”€ opena20 (Dashboard) â†’ scraped LocalAgent Metrics
```

**Vorteile:**
- âœ… UnabhÃ¤ngige Skalierung
- âœ… Keine AbhÃ¤ngigkeiten zu ELION
- âœ… Einfache Wartung

### Szenario 2: Full Integration (opena21)

```
ELION-System
    â”œâ”€â”€ opena1 (Koordinator) â†’ dispatched an opena21
    â”œâ”€â”€ opena2 (Archivator) â†’ empfÃ¤ngt Safepoints von opena21
    â”œâ”€â”€ opena21 (LocalAgent-Pro auf 12364)
    â”‚       â”œâ”€â”€ Ollama Integration
    â”‚       â”œâ”€â”€ Tool Execution
    â”‚       â””â”€â”€ Sandbox Isolation
    â”œâ”€â”€ opena3 (OpenWebUI) â†’ nutzt opena21 via Koordinator
    â”œâ”€â”€ opena4 (Telegram) â†’ leitet AI-Requests an opena21
    â””â”€â”€ opena20 (Dashboard) â†’ monitored opena21 Metriken
```

**Vorteile:**
- âœ… Zentrale Orchestrierung via opena1
- âœ… Alle Aktionen protokolliert in opena2
- âœ… Einheitliches Routing
- âœ… VollstÃ¤ndige ELION-Integration

### Szenario 3: Hybrid

LocalAgent-Pro lÃ¤uft auf Port 8001, aber:
- opena1 kann bei Bedarf AI-Tasks dorthin routen
- opena4 (Telegram) nutzt LocalAgent fÃ¼r Chat
- opena20 scraped Metriken
- Safepoints werden optional an opena2 gesendet

---

## ğŸ“ Beispiel-Workflows

### Workflow 1: Telegram â†’ AI â†’ Archiv

```
1. User sendet Telegram-Nachricht
   â†“
2. opena4 empfÃ¤ngt â†’ schreibt in opena2
   â†“
3. opena4 ruft LocalAgent-Pro (POST /v1/chat/completions)
   â†“
4. LocalAgent fÃ¼hrt Tools aus (Sandbox)
   â†“
5. LocalAgent sendet Antwort zurÃ¼ck
   â†“
6. opena4 archiviert Antwort in opena2
   â†“
7. opena4 sendet Telegram-Reply
```

### Workflow 2: OpenWebUI â†’ Direct

```
1. User chattet in OpenWebUI (opena3)
   â†“
2. OpenWebUI â†’ http://localhost:8001/v1/chat/completions
   â†“
3. LocalAgent-Pro verarbeitet
   â†“
4. Antwort direkt an OpenWebUI
   
(Optional: LocalAgent sendet Safepoint an opena2)
```

### Workflow 3: Koordinator-Dispatch

```
1. opena1 erhÃ¤lt komplexen Auftrag
   â†“
2. opena1 erkennt: "Braucht AI-Inference"
   â†“
3. opena1 â†’ POST http://localhost:8001/v1/chat/completions
   â†“
4. LocalAgent fÃ¼hrt aus + sendet Ergebnis
   â†“
5. opena1 â†’ schreibt Safepoint in opena2
   â†“
6. opena1 orchestriert weitere Schritte
```

---

## ğŸ¯ Empfehlung

**FÃ¼r dein Setup:**

1. **Behalte Port 8001** (Standalone) â†’ Einfachste Integration
2. **Erweitere opena20** â†’ Scraped bereits LocalAgent-Metriken âœ…
3. **Optional: opena4 Integration** â†’ Telegram-Bot nutzt LocalAgent fÃ¼r AI
4. **SpÃ¤ter: Als opena21 registrieren** â†’ FÃ¼r vollstÃ¤ndige ELION-Integration

**NÃ¤chste Schritte:**
1. âœ… Prometheus-Integration lÃ¤uft bereits
2. ğŸ“Š Grafana-Dashboard fÃ¼r LocalAgent erstellen
3. ğŸ¤– opena4 (Telegram) mit LocalAgent verbinden
4. ğŸ“‹ Safepoint-Integration in opena2 implementieren

**Dein System ist bereits production-ready mit vollem Monitoring!** ğŸš€
