# ğŸš€ LocalAgent-Pro API Endpoint Cheatsheet (Linux Mint)

## ğŸ“‹ SchnellÃ¼bersicht

| Endpoint | Methode | Beschreibung |
|----------|---------|--------------|
| `/` | GET | HTML-Ãœbersicht |
| `/health` | GET | Server-Status & Konfiguration |
| `/whitelist` | GET | Auto-Whitelist anzeigen |
| `/v1` | GET | API-Versionsinformationen |
| `/v1/models` | GET | VerfÃ¼gbare Modelle |
| `/v1/chat/completions` | POST | Chat API (OpenAI-kompatibel) |
| `/test` | GET/POST | Tool-Test-Endpoint |

---

## ğŸ§ª Schnell-Tests (Copy & Paste)

### 1ï¸âƒ£ Server lÃ¤uft?

```bash
curl -s http://127.0.0.1:8001/health | jq '.status'
# Output: "ok"
```

### 2ï¸âƒ£ Welche Modelle verfÃ¼gbar?

```bash
curl -s http://127.0.0.1:8001/v1/models | jq '.data[].id'
# Output: "localagent-pro" + "llama3.1" (oder dein LLM_MODEL)
```

### 3ï¸âƒ£ Sandbox aktiv?

```bash
curl -s http://127.0.0.1:8001/health | jq '.sandbox'
# Output: false (bei dir deaktiviert)
```

### 4ï¸âƒ£ Wildcard-Domains aktiv?

```bash
curl -s http://127.0.0.1:8001/health | jq '.allowed_domains'
# Output: ["*"]
```

### 5ï¸âƒ£ Whitelist anzeigen

```bash
curl -s http://127.0.0.1:8001/whitelist | jq '.'
# Zeigt alle auto-genehmigten Domains
```

---

## ğŸ’¬ Chat API Tests

### âœï¸ Datei erstellen

```bash
curl -X POST http://127.0.0.1:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "localagent-pro",
    "messages": [{"role": "user", "content": "Erstelle demo.txt mit Hello World"}],
    "stream": false
  }' | jq -r '.choices[0].message.content'
```

### ğŸ“– Datei lesen

```bash
curl -X POST http://127.0.0.1:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "localagent-pro",
    "messages": [{"role": "user", "content": "Lies demo.txt"}],
    "stream": false
  }' | jq -r '.choices[0].message.content'
```

### ğŸ—‘ï¸ Datei lÃ¶schen

```bash
curl -X POST http://127.0.0.1:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "localagent-pro",
    "messages": [{"role": "user", "content": "LÃ¶sche demo.txt"}],
    "stream": false
  }' | jq -r '.choices[0].message.content'
```

### ğŸ“‚ Dateien auflisten

```bash
curl -X POST http://127.0.0.1:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "localagent-pro",
    "messages": [{"role": "user", "content": "Liste alle Dateien auf"}],
    "stream": false
  }' | jq -r '.choices[0].message.content'
```

### ğŸŒ Web-Request (nur mit Wildcard)

```bash
curl -X POST http://127.0.0.1:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "localagent-pro",
    "messages": [{"role": "user", "content": "Hole github.com"}],
    "stream": false
  }' | jq -r '.choices[0].message.content'
```

---

## ğŸ”¬ Test-Endpoint (Direkter Tool-Zugriff)

### GET-Methode

```bash
curl -s "http://127.0.0.1:8001/test?prompt=Erstelle%20quick.txt%20mit%20Quick%20Test" | jq '.'
```

### POST-Methode

```bash
curl -X POST http://127.0.0.1:8001/test \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Lies quick.txt"}' | jq '.'
```

---

## ğŸ“¡ Streaming-Test

```bash
curl -N -X POST http://127.0.0.1:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "localagent-pro",
    "messages": [{"role": "user", "content": "Erstelle stream.txt mit Streaming funktioniert"}],
    "stream": true
  }'
```

---

## ğŸ¯ OpenWebUI Integration

### Konfiguration in OpenWebUI

1. Ã–ffne OpenWebUI: `http://localhost:3000`
2. Gehe zu: **Settings â†’ Connections**
3. FÃ¼ge hinzu:
   - **API Base URL**: `http://127.0.0.1:8001/v1`
   - **API Key**: (leer lassen oder beliebig)
4. Speichern & Testen

### Testen in OpenWebUI

```
User: Erstelle test.txt mit OpenWebUI funktioniert!
Agent: âœ… Datei erstellt (Live: /absolute/path/test.txt)
      ğŸ“ 31 Zeichen geschrieben
```

---

## ğŸ§ Linux Mint Spezifisch

### Server starten

```bash
cd /home/danijel-jd/Dokumente/Workspace/Projekte/Lokales\ Agententool/LocalAgent-Pro
./start_server.sh
```

### Server neustarten

```bash
./restart_server.sh
```

### Server stoppen

```bash
ps aux | grep "python.*openwebui_agent_server" | grep -v grep | awk '{print $2}' | xargs -r kill
```

### Logs anzeigen

```bash
tail -f logs/app.log
# oder fÃ¼r Ollama-Logs:
tail -f logs/ollama.log
```

### VollstÃ¤ndiger Endpoint-Test

```bash
./TEST_ENDPOINTS.sh
```

---

## ğŸ” Debugging

### PrÃ¼fe ob Server lÃ¤uft

```bash
curl -s http://127.0.0.1:8001/health && echo "âœ… Server lÃ¤uft" || echo "âŒ Server offline"
```

### PrÃ¼fe Port 8001

```bash
netstat -tulpn | grep :8001
# oder mit ss:
ss -tulpn | grep :8001
```

### PrÃ¼fe Prozess

```bash
ps aux | grep openwebui_agent_server
```

### Log-Level Ã¤ndern (temporÃ¤r)

In `src/openwebui_agent_server.py` Zeile 24:

```python
log_level="DEBUG",  # DEBUG | INFO | WARNING | ERROR
```

---

## âš™ï¸ Konfiguration anpassen

### Sandbox deaktivieren (bereits erledigt)

`config/config.yaml`:

```yaml
sandbox: false  # âœ… Bereits deaktiviert
```

### Wildcard-Domains (bereits aktiv)

`config/config.yaml`:

```yaml
allowed_domains:
  - "*"  # âœ… Bereits aktiv
```

### Auto-Whitelist aktivieren

`config/config.yaml`:

```yaml
auto_whitelist_enabled: true  # Domains automatisch speichern
auto_whitelist_file: "config/domain_whitelist.yaml"
```

### LLM-Modell Ã¤ndern

`config/config.yaml`:

```yaml
llm:
  model: "llama3.1"  # Ã„ndern zu anderem Ollama-Modell
```

---

## ğŸ¨ Erweiterte Beispiele

### Marker-Pattern (exakte Code-Ãœbergabe)

```bash
curl -X POST http://127.0.0.1:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "localagent-pro",
    "messages": [{
      "role": "user", 
      "content": "Erstelle advanced.py mit <<<CONTENT\nimport sys\nprint(f\"Python: {sys.version}\")\n<<<END"
    }],
    "stream": false
  }' | jq -r '.choices[0].message.content'
```

### Mehrere Tools in einer Anfrage

```bash
curl -X POST http://127.0.0.1:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "localagent-pro",
    "messages": [{
      "role": "user", 
      "content": "Erstelle multi.txt mit Multiple Tools, dann lies die Datei"
    }],
    "stream": false
  }' | jq -r '.choices[0].message.content'
```

---

## ğŸ“Š Response-Struktur

### Health Check Response

```json
{
  "status": "ok",
  "server_time": 1731763200,
  "model": "llama3.1",
  "sandbox": false,
  "sandbox_path": "~/localagent_sandbox",
  "allowed_domains": ["*"],
  "auto_whitelist_enabled": true,
  "auto_whitelist_count": 4,
  "open_webui_port": 3000
}
```

### Chat Completion Response

```json
{
  "id": "chatcmpl-a1b2c3d4",
  "object": "chat.completion",
  "created": 1731763200,
  "model": "localagent-pro",
  "choices": [{
    "index": 0,
    "finish_reason": "stop",
    "message": {
      "role": "assistant",
      "content": "ğŸ¤– LocalAgent-Pro hat deine Anfrage bearbeitet:\n\nâœï¸ Datei schreiben:\nâœ… Datei erstellt (Live: /path/to/file.txt)\nğŸ“ 25 Zeichen geschrieben"
    }
  }],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 25,
    "total_tokens": 35
  }
}
```

---

## ğŸš¨ Fehlerbehandlung

### 400 Bad Request

```bash
# Fehler: Leerer Prompt
curl -X POST http://127.0.0.1:8001/test \
  -H "Content-Type: application/json" \
  -d '{"prompt": ""}'

# Response:
{
  "error": "Kein Prompt angegeben",
  "hint": "Sende GET mit ?prompt=... oder POST mit {\"prompt\": \"...\"}",
  "examples": {
    "GET": "/test?prompt=Lies%20demo.py",
    "POST": "{\"prompt\": \"Erstelle demo.py mit print('Hello')\"}"
  }
}
```

### 500 Internal Server Error

```bash
# Server-Logs prÃ¼fen:
tail -f logs/app.log

# Typische Ursachen:
# - Ollama nicht erreichbar
# - UngÃ¼ltige Konfiguration
# - Fehlende Berechtigungen
```

---

## âœ… Checkliste fÃ¼r Production

- [ ] Server lÃ¤uft: `curl http://127.0.0.1:8001/health`
- [ ] Sandbox-Status geprÃ¼ft: `sandbox: false` âœ…
- [ ] Domains konfiguriert: Wildcard `*` âœ…
- [ ] Logging aktiv: `logs/app.log` existiert
- [ ] Ollama erreichbar: `ollama list`
- [ ] OpenWebUI verbunden: API Base URL gesetzt
- [ ] Auto-Whitelist funktioniert: `curl /whitelist`
- [ ] Marker-Pattern getestet: `<<<CONTENT...<<<END` âœ…

---

**ğŸ¯ Tipp fÃ¼r Linux Mint:**
FÃ¼ge den Server zu Autostart hinzu:

```bash
# ~/.config/autostart/localagent-pro.desktop
[Desktop Entry]
Type=Application
Name=LocalAgent-Pro Server
Exec=/home/danijel-jd/Dokumente/Workspace/Projekte/Lokales Agententool/LocalAgent-Pro/start_server.sh
Terminal=true
```

---

**Erstellt:** 16. November 2025  
**System:** Linux Mint 22.2  
**Server:** LocalAgent-Pro v1.0
