#!/usr/bin/env python3
"""
Ollama-Integration Test fÃ¼r LocalAgent-Pro
Testet die Verbindung und Funktionen mit dem verfÃ¼gbaren Modell
"""

import sys
import os

# Pfad zum src-Verzeichnis hinzufÃ¼gen
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from ollama_integration import OllamaClient
from logging_config import get_logging_manager

# Logger erstellen
logging_manager = get_logging_manager()
logger = logging_manager.get_logger("OllamaTest")

def main():
    logger.info("=" * 80)
    logger.info("ğŸ§ª Ollama-Integration VollstÃ¤ndiger Test")
    logger.info("=" * 80)
    
    # 1. Client erstellen
    logger.info("\nğŸ“‹ Schritt 1: Ollama-Client erstellen...")
    client = OllamaClient(
        base_url="http://127.0.0.1:11434",
        timeout=60,
        default_model="llama2:latest"  # VerfÃ¼gbares Modell
    )
    
    # 2. Modelle auflisten
    logger.info("\nğŸ“‹ Schritt 2: VerfÃ¼gbare Modelle auflisten...")
    models = client.list_models()
    
    if models:
        print(f"\nâœ… {len(models)} Modelle gefunden:")
        for model in models:
            name = model.get("name", "unknown")
            size = model.get("size", 0) / (1024**3)  # GB
            modified = model.get("modified_at", "unknown")
            print(f"  ğŸ“¦ {name} ({size:.2f} GB) - Modified: {modified}")
    else:
        print("\nâŒ Keine Modelle gefunden!")
        return
    
    # 3. Einfache Generate-API testen
    logger.info("\nğŸ“‹ Schritt 3: Generate-API testen...")
    
    # Teste mit direktem API-Call
    import requests
    
    generate_url = "http://127.0.0.1:11434/api/generate"
    generate_payload = {
        "model": "llama2:latest",
        "prompt": "Was ist Python? Antworte in einem Satz.",
        "stream": False
    }
    
    logger.info(f"ğŸ” Teste: POST {generate_url}")
    logger.debug(f"ğŸ“¦ Payload: {generate_payload}")
    
    try:
        response = requests.post(generate_url, json=generate_payload, timeout=30)
        logger.info(f"ğŸ“Š Status Code: {response.status_code}")
        
        if response.status_code == 200:
            result = response.json()
            generated_text = result.get("response", "")
            print(f"\nâœ… Generate-API erfolgreich!")
            print(f"ğŸ’¬ Antwort: {generated_text}")
            
            # Statistiken
            eval_count = result.get("eval_count", 0)
            eval_duration = result.get("eval_duration", 0) / 1e9  # ns zu s
            tokens_per_sec = eval_count / eval_duration if eval_duration > 0 else 0
            
            print(f"ğŸ“Š Tokens: {eval_count}")
            print(f"â±ï¸  Dauer: {eval_duration:.2f}s")
            print(f"ğŸš€ Speed: {tokens_per_sec:.1f} tokens/s")
        else:
            print(f"\nâŒ Generate-API Fehler: {response.status_code}")
            print(f"ğŸ“„ Response: {response.text}")
    except Exception as e:
        logger.error(f"âŒ Generate-API Exception: {e}", exc_info=True)
        print(f"\nâŒ Fehler: {e}")
    
    # 4. Chat-API testen
    logger.info("\nğŸ“‹ Schritt 4: Chat-API testen...")
    
    chat_url = "http://127.0.0.1:11434/api/chat"
    chat_payload = {
        "model": "llama2:latest",
        "messages": [
            {"role": "system", "content": "Du bist ein hilfreicher Assistent."},
            {"role": "user", "content": "ErklÃ¤re Docker in einem Satz."}
        ],
        "stream": False
    }
    
    logger.info(f"ğŸ” Teste: POST {chat_url}")
    logger.debug(f"ğŸ“¦ Payload: {chat_payload}")
    
    try:
        response = requests.post(chat_url, json=generate_payload, timeout=30)
        logger.info(f"ğŸ“Š Status Code: {response.status_code}")
        
        if response.status_code == 200:
            result = response.json()
            message = result.get("message", {})
            chat_text = message.get("content", "")
            print(f"\nâœ… Chat-API erfolgreich!")
            print(f"ğŸ’¬ Antwort: {chat_text}")
        else:
            print(f"\nâŒ Chat-API Fehler: {response.status_code}")
            print(f"ğŸ“„ Response: {response.text}")
    except Exception as e:
        logger.error(f"âŒ Chat-API Exception: {e}", exc_info=True)
        print(f"\nâŒ Fehler: {e}")
    
    # 5. Model-Info testen
    logger.info("\nğŸ“‹ Schritt 5: Model-Info abrufen...")
    
    model_info = client.get_model_info("llama2:latest")
    if model_info:
        print(f"\nâœ… Model-Info abgerufen:")
        print(f"ğŸ“Š Template: {model_info.get('template', 'N/A')[:100]}...")
        print(f"ğŸ“Š Parameters: {model_info.get('parameters', 'N/A')[:100]}...")
    else:
        print(f"\nâš ï¸  Model-Info konnte nicht abgerufen werden")
    
    logger.info("\n" + "=" * 80)
    logger.info("âœ… Ollama-Integration Test abgeschlossen!")
    logger.info("=" * 80)

if __name__ == "__main__":
    main()
