# ü§ñ OpenWebUI-Integration Copilot Prompt

## System-Prompt f√ºr VSCode/OpenWebUI Copilot

```
Du bist ein AI-Assistant f√ºr LocalAgent-Pro mit OpenWebUI-Integration.

BACKEND-API:
- Base URL: http://127.0.0.1:8001/v1
- Port: 8001 (Backend API)

ENDPOINTS:
- POST http://127.0.0.1:8001/v1/chat/completions (Chat)
- GET  http://127.0.0.1:8001/v1/models (Modelliste)
- POST http://127.0.0.1:8001/test (Tool-Test)
- GET  http://127.0.0.1:8001/health (Status)

OPENWEBUI UI:
- Port: 3000 (separates Web-Interface)
- Verbindet sich mit Backend API √ºber http://127.0.0.1:8001/v1

WICHTIG:
- /v1 alleine gibt 404 (normal) - nutze vollst√§ndige Pfade
- API Base URL in OpenWebUI: http://127.0.0.1:8001/v1
- Bei Problemen: Pr√ºfe /health, /v1/models, /v1/chat/completions
```

---

## Anweisungen f√ºr Copilot

### 1. Server-Status pr√ºfen
```bash
curl -s http://127.0.0.1:8001/health
```

### 2. Modelle pr√ºfen
```bash
curl -s http://127.0.0.1:8001/v1/models
```

### 3. Chat-Endpoint testen
```bash
curl -s -X POST \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"Hallo LocalAgent-Pro"}]}' \
  http://127.0.0.1:8001/v1/chat/completions
```

### 4. Bei 404-Fehler
- ‚úÖ Nutze vollst√§ndigen Pfad: `/v1/chat/completions` statt `/v1`
- ‚úÖ Pr√ºfe ob Server l√§uft: `curl http://127.0.0.1:8001/health`
- ‚úÖ √úberpr√ºfe Port: `ss -tlnp | grep 8001`

### 5. OpenWebUI konfigurieren
1. √ñffne: http://localhost:3000
2. Settings ‚Üí Connections ‚Üí OpenAI API
3. **API Base URL**: `http://127.0.0.1:8001/v1`
4. **API Key**: `dummy`
5. Save & Test

---

## Erwartete Ausgaben

### Health Check
```json
{
  "status": "ok",
  "model": "llama3.1",
  "sandbox": true,
  "sandbox_path": "/home/danijel-jd/localagent_sandbox"
}
```

### Models
```json
{
  "object": "list",
  "data": [
    {"id": "localagent-pro", "object": "model"},
    {"id": "llama3.1", "object": "model"}
  ]
}
```

### Chat Response
```json
{
  "id": "chatcmpl-...",
  "object": "chat.completion",
  "choices": [{
    "message": {
      "role": "assistant",
      "content": "ü§ñ LocalAgent-Pro: ..."
    }
  }]
}
```

---

## Troubleshooting Checkliste

- [ ] Backend l√§uft auf Port 8001
- [ ] OpenWebUI l√§uft auf Port 3000
- [ ] API Base URL korrekt: `http://127.0.0.1:8001/v1`
- [ ] Health-Endpoint antwortet
- [ ] Models-Endpoint listet Modelle
- [ ] Chat-Endpoint funktioniert
- [ ] Keine Firewall-Blockierung

---

## Quick Fix Commands

```bash
# Server starten
cd LocalAgent-Pro && ./start_server.sh

# Server stoppen
./stop_server.sh

# Vollst√§ndiger Check
./openwebui_check.sh

# Logs pr√ºfen
tail -f server.log

# Port-Status
ss -tlnp | grep -E "8001|3000"
```

---

## Port-√úbersicht

| Service | Port | URL |
|---------|------|-----|
| LocalAgent-Pro API | 8001 | http://127.0.0.1:8001/v1 |
| OpenWebUI UI | 3000 | http://127.0.0.1:3000 |

**Wichtig**: Verwende Port 8001 f√ºr API-Verbindungen, Port 3000 f√ºr Browser-Zugriff!
