# ğŸ¤– LocalAgent-Pro - VollstÃ¤ndige Installation

Ein KI-gestÃ¼tzter Agent mit Ollama-Integration, GPU-Beschleunigung und OpenWebUI-KompatibilitÃ¤t.

## ğŸš€ Schnellstart (Ein Befehl)

```bash
./setup_localagent_pro.sh
```

Dieses Skript:
- âœ… PrÃ¼ft System-Anforderungen (Python, GPU, Ollama)
- âœ… Erstellt Python Virtual Environment
- âœ… Installiert alle AbhÃ¤ngigkeiten
- âœ… LÃ¤dt Ollama-Modell herunter (tinyllama)
- âœ… Konfiguriert GPU-Beschleunigung (optional)
- âœ… Erstellt Konfiguration und Logs
- âœ… FÃ¼hrt System-Tests durch

---

## ğŸ“‹ Voraussetzungen

### Minimum:
- **OS**: Linux (Ubuntu 20.04+, Linux Mint 21+)
- **Python**: 3.8+
- **RAM**: 8 GB
- **Speicher**: 5 GB frei

### Empfohlen:
- **GPU**: NVIDIA (GTX 1050 oder besser, 4+ GB VRAM)
- **CUDA**: 12.0+
- **RAM**: 16 GB
- **Speicher**: 10 GB frei

---

## ğŸ› ï¸ Manuelle Installation

### 1. Ollama installieren

```bash
curl -fsSL https://ollama.com/install.sh | sh
sudo systemctl enable ollama
sudo systemctl start ollama
```

### 2. Modell herunterladen

```bash
# Schnelles Modell fÃ¼r GPU (empfohlen)
ollama pull tinyllama

# Oder grÃ¶ÃŸeres Modell
ollama pull llama2
```

### 3. Python-Umgebung einrichten

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 4. GPU-Beschleunigung (optional)

```bash
./setup_gpu_acceleration.sh
```

### 5. Konfiguration anpassen

Bearbeite `config/config.yaml`:

```yaml
llm:
  model: "tinyllama"
  api_base: "http://127.0.0.1:11434"

sandbox: true
sandbox_path: "~/localagent_sandbox"

allowed_domains:
  - "example.com"
  - "github.com"
```

---

## ğŸ¯ Server starten

### Option 1: Start-Skript (empfohlen)

```bash
./start_server.sh
```

### Option 2: Manuell

```bash
source venv/bin/activate
python3 src/openwebui_agent_server.py
```

### Option 3: Hintergrund (daemonisiert)

```bash
nohup python3 src/openwebui_agent_server.py > server.log 2>&1 &
```

---

## ğŸ§ª Tests durchfÃ¼hren

### Ollama-Integration testen

```bash
python3 quick_test_ollama.py
```

### GPU-Performance messen

```bash
python3 benchmark_cpu_vs_gpu.py
```

### API-Endpoints testen

```bash
python3 test_api_endpoints.py
```

### Manuelle API-Tests

```bash
# Health-Check
curl -s http://127.0.0.1:8001/health | jq '.'

# Modelle
curl -s http://127.0.0.1:8001/v1/models | jq '.'

# Chat (non-streaming)
curl -X POST http://127.0.0.1:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"Hallo!"}]}' | jq '.'

# Chat (streaming)
curl -N -X POST http://127.0.0.1:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"ZÃ¤hle von 1 bis 5"}],"stream":true}'
```

---

## ğŸŒ OpenWebUI-Integration

### 1. OpenWebUI Ã¶ffnen

```
http://localhost:3000
```

### 2. API konfigurieren

- Gehe zu: **Settings** â†’ **Connections** â†’ **OpenAI API**
- **API Base URL**: `http://127.0.0.1:8001/v1`
- **API Key**: (leer lassen oder beliebig)
- **Modell wÃ¤hlen**: `tinyllama` oder `localagent-pro`

### 3. Testen

Schreibe eine Nachricht im Chat und beobachte:
- âœ… Streaming-Antworten
- âœ… Tool-Aufrufe (Dateien, Shell, Web)
- âœ… GPU-beschleunigte Inferenz

---

## ğŸ“Š Monitoring & Logs

### Live-Logs anzeigen

```bash
./tail_logs.sh
```

### Log-Analyse

```bash
./analyze_logs.sh
```

### Logs aufrÃ¤umen

```bash
./cleanup_logs.sh
```

### GPU-Ãœberwachung

```bash
# Echtzeit
watch -n 1 nvidia-smi

# Ollama-Logs
sudo journalctl -u ollama -f
```

---

## ğŸ“ Projekt-Struktur

```
LocalAgent-Pro/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ openwebui_agent_server.py  # Haupt-Backend-Server
â”‚   â”œâ”€â”€ ollama_integration.py      # Ollama-Client
â”‚   â””â”€â”€ logging_config.py          # Logging-System
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml                # Konfiguration
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ localagent-pro.log         # Haupt-Log
â”‚   â”œâ”€â”€ api_requests.log           # API-Requests
â”‚   â”œâ”€â”€ tool_executions.log        # Tool-Aufrufe
â”‚   â””â”€â”€ ollama_integration.log     # Ollama-Calls
â”œâ”€â”€ setup_localagent_pro.sh        # VollstÃ¤ndiges Setup
â”œâ”€â”€ start_server.sh                # Server starten
â”œâ”€â”€ setup_gpu_acceleration.sh      # GPU-Setup
â”œâ”€â”€ quick_test_ollama.py           # Ollama-Test
â”œâ”€â”€ benchmark_cpu_vs_gpu.py        # Performance-Test
â”œâ”€â”€ test_api_endpoints.py          # API-Tests
â”œâ”€â”€ requirements.txt               # Python-AbhÃ¤ngigkeiten
â””â”€â”€ *.md                           # Dokumentation
```

---

## ğŸ› ï¸ VerfÃ¼gbare Tools

Der Agent unterstÃ¼tzt folgende Tools:

### 1. **Datei lesen**
```
"Lies Datei test.txt"
"Zeige config.yaml"
```

### 2. **Datei schreiben**
```
"Erstelle Datei hello.txt mit Hallo Welt"
"Schreibe test.py mit print('Hello')"
```

### 3. **Verzeichnis auflisten**
```
"Liste alle Dateien auf"
"Zeige Dateien im Ordner /tmp"
```

### 4. **Shell-Kommando**
```
"FÃ¼hre Kommando 'ls -la' aus"
"Execute 'pwd'"
```

### 5. **Web-Request**
```
"Hole github.com"
"Lade Webseite example.com"
```

---

## ğŸš€ Performance-Optimierung

### GPU-Beschleunigung aktivieren

```bash
./setup_gpu_acceleration.sh
```

**Erwartete Verbesserung:**
- CPU-Modus: ~2-3 tokens/s
- GPU-Modus: ~20-30 tokens/s (mit tinyllama)
- **Speedup: 9-10x schneller!**

### Modell-Auswahl (GPU GTX 1050, 4GB VRAM)

| Modell | VRAM | Speed | QualitÃ¤t |
|--------|------|-------|----------|
| `tinyllama` | 637 MB | âš¡âš¡âš¡ | â­â­ |
| `phi3:mini` | 2.3 GB | âš¡âš¡ | â­â­â­ |
| `llama2:7b` | 3.8 GB | âš¡ | â­â­â­â­ |

### Konfiguration anpassen

In `config/config.yaml`:

```yaml
llm:
  model: "tinyllama"  # Schnellstes Modell
  temperature: 0.7    # KreativitÃ¤t (0.0-1.0)
  max_tokens: 500     # Max. Response-LÃ¤nge
```

---

## ğŸ› Troubleshooting

### Problem: "Ollama nicht erreichbar"

```bash
# Service prÃ¼fen
systemctl status ollama

# Service starten
sudo systemctl start ollama

# Manuell starten
ollama serve
```

### Problem: "Port 8001 bereits belegt"

```bash
# Prozess finden
lsof -i:8001

# Prozess beenden
ps aux | grep openwebui_agent_server | awk '{print $2}' | xargs kill
```

### Problem: "GPU wird nicht genutzt"

```bash
# GPU-Status prÃ¼fen
nvidia-smi

# CUDA-Version prÃ¼fen
nvcc --version

# Ollama-Logs prÃ¼fen
sudo journalctl -u ollama -n 50
```

### Problem: "Import-Fehler"

```bash
# Virtual Environment aktivieren
source venv/bin/activate

# AbhÃ¤ngigkeiten neu installieren
pip install -r requirements.txt --force-reinstall
```

---

## ğŸ“š Dokumentation

- **GPU_SETUP.md** - GPU-Beschleunigung einrichten
- **OLLAMA_SETUP.md** - Ollama-Integration
- **LOGGING_GUIDE.md** - Logging-System
- **LOGGING_IMPLEMENTATION.md** - Tech-Details

---

## ğŸ”— NÃ¼tzliche Links

- [Ollama](https://ollama.com/)
- [OpenWebUI](https://github.com/open-webui/open-webui)
- [NVIDIA CUDA](https://developer.nvidia.com/cuda-toolkit)

---

## ğŸ“ Lizenz

MIT License - Siehe LICENSE-Datei

---

## ğŸ¤ Support

Bei Problemen:
1. PrÃ¼fe die Logs: `./tail_logs.sh`
2. FÃ¼hre Tests aus: `python3 test_api_endpoints.py`
3. Lies die Dokumentation: `cat GPU_SETUP.md`

---

**Erstellt**: 16. November 2025  
**Version**: 1.0.0  
**Hardware getestet**: NVIDIA GeForce GTX 1050 (4GB VRAM)  
**OS getestet**: Linux Mint 22.2
